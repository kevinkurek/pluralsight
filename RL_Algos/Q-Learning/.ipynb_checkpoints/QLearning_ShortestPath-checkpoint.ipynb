{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx\n",
    "# !pip install pygraphviz\n",
    "# !brew install graphviz (might have to do this first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Undirect 9 node graph labeled 0-8\n",
    "edge_list = [(0,2), (0,1), (0,3), (2,4), (5,6), (7,4), (0,6), (5,3), (3,7), (0,8)]\n",
    "# weights = [1, 1, 5, 8, 3, 6, 9, 2, 3, 4]\n",
    "weights = [.1, .1, .5, .8, .3, .6, .9, .2, .3, .4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVxU9f748dcMwyogirgCYVIgKi64kCu54YJKfjU1LbNFTVu8ldrN6udNvZpbdl2zzDJvaWoKF1FxQ8UrrikqgqKioEACIusMs5zfH1ymiEVF4AzD5/l4+EjPnDm8p4E3n3mfz+f9UUiShCAIglAzlHIHIAiCUJeIpCsIglCDRNIVBEGoQSLpCoIg1CCRdAVBEGqQqqIHGzVqJHl4eNRQKIIgCObh7Nmz6ZIkuZT1WIVJ18PDgzNnzlRPVGYoPVfD9rPJxKVmk63W4WijwrupI6P9XHG2t5Y7PEEQaohCobhV3mMVJl3h0VxIymJ1ZAJHrt4DQKMzGB+zUaXy5YGrBHi5MK2PJ+3dnOQKUxAEEyCS7hPaHJ3IgvA41Do9Za0zUf8vAUfEpnH0ajpzhngzwd+jZoMUBMFkiKT7BIoS7hUKtIaHnitJUKDVsyD8CoBIvIJQR4nZC5V0ISmLBeFxj5Rw/6xAa2BBeBwxyVnVFJkgCKZMjHQraXVkAmqdvtRxXVYaGRFrKLwTBypL6nn1oEH/ySiUFsZz1Do9ayITWDehc02GLAiCCRAj3UpIz9Vw5Oq9Mmu4GRFrsLBzwvWdH2k+aSXqpEvknNtd4hxJgsPx98jI1dRQxIIgmAqRdCth+9nkch/TPUijXuueKFRWWNg3wLalH9r026XOUwDbz5V/HUEQzJNIupUQl5pdYlrYnzl2Hk5e7FEMWjW6nHQKbpzBtmWnUuepdQbiUnKqO1RBEEyMqOlWQrZaV+5jNm7tyD2/j6TlL4JkoF7bftg++1w519FWV4iCIJgoMdKtBEebsn9XSZKBtF8+w86rO+4f7MD1vZ8wqHPJitxYznUsqzNMQRBMkEi6leDd1BFrVen/dYaCHPTZ93DoFIRCZYmFrSP2vv0puF56KbWNSol3M4eaCFcQBBMikm4ljPJzLfO4hV19VPWbkPNbOJJBj0GdS+7Fg1g2blnqXAkY1ans6wiCYL5ETbcSGtlb0+dZF/ZfSSs1bcxl5BwyD6wnO3o7KC2wcW9Hw35vljhHoYDnvVxEE5w6TjRIqptE0q2k6QGeHLuWToG25AIJqyZP03T8ogqfa6OyYFqAZ3WGJ5gw0SCpbhPlhUpq7+bEnCHe2Fo+3v9CG5WSOUO88XUVP0x10eboRMZ+E83+K2lodIZSUw/V/zsWEZvG2G+i2RydKE+gQrURSfcJTPD3YM6Q1thaWqBQVHyuQgEW6LG5spsXOzWvmQAFk/JHg6SyO9L92Z8bJInEa15E0n1CE/w92DrZn0CfJlirlNj8ZVaDjUqJtUpJoE8TdrzVCw9dMm+//TbSw37qBLNSUYOkvNgj3PlmKreX/R931r2BOumS8THRIMn8KCr64e/cubMkdo54dBm5GrafSyYuJYdstRZHG0u8mzkwqtMfN0ZycnJ47rnnmD59Om+99ZbMEQs1ZfKPZ8q88Vpw8zcy9vwLlxGzsWr+LPrcTABUDo2M5ygUEOjTRDRIqkUUCsVZSZLKfMPEjbQq5GxvzZTerSo8x8HBgZCQELp3746Pjw99+vSpoegEuVTUIOlB1L+p32Mc1i28gZLJttifGySJWQ21nygvyKBVq1Zs3ryZsWPHcutWuVspCWaivAZJkkGPJiUBQ/4D7qx7k+TVE8mMWItBW7r7nGiQZD5E0pXJgAEDmDVrFsHBweTl5ckdjlCNymuQpM/LAoOO/PjjNJnwBc0m/YvCtBs8+O/WUueKBknmQyRdGc2YMQNfX19ee+01cWPNjJXXIElhWVQqcPAbhsq+IRZ29XHoElzmsvGi64gGSeZAJF0ZKRQKvv76a27evMmiRRUvqBBqr/IaJFnY2GNRRg23/OuIBknmQNxIk5mNjQ07d+6ka9eutGvXjqCgILlDEqpYUYOk1DJLDPbt+pNzNgzbp/3AQkXOmRDsPLuUOk/SaQj/+RscLjszfPhw/Pz8UDxscrhgksRI1wS0aNGC7du389prr3HlyhW5wxGq2Cg/V8orHtXvMRarZs9wZ/0U7n4zFasmrajffUyp82xsbPnynTGo1WrGjx+Pq6srb731Fnv37kWjEds+1SZinq4J2bhxIwsXLuTUqVM4OYllwuYiIiKCKZvPIjVvC4rHH+eUNU83Pj6e0NBQQkJCuHjxIgMGDGDEiBEMGTIEZ2fnqgxfqISK5umKka4JmTRpEoMGDWLcuHHo9aV3GhZql7t37zJ27FimTJnCzCG+2FpVriZbVoMkLy8vZs6cSVRUFNeuXWPo0KHs2LGDli1bEhAQwJdffsn169er4mUIVUwkXROzbNkyNBoNc+bMkTsUoZL0ej0rV66kffv2tGrVisuXLzNt7NBKNUiytXx4g6TGjRszadIkdu3aRVpaGh988AGxsbH06NGDtm3b8vHHH3Py5EkMhrL39RNqlriRZmIsLS355Zdf6Nq1K+3bt2fcuHFyhyQ8htOnTzN16lQcHBw4evQorVu3Nj42wd8DgAXhcah1FTe9USiKRrhzhngbn/cobG1tGTZsGMOGDcNgMHDq1ClCQkJ47bXXyMzMZNiwYQwfPpx+/fpha2tbyVcpPAlR0zVRMTEx9OvXj7179+Ln5yd3OMJDZGVlMWfOHHbs2MGSJUuYMGFCubMLYpKzWBOZwOH4eygoWvhQzEalRKKoyf20AM8qbQGakJBgrAOfP3+evn37MmLECIYOHYqLi0uVfR2h4pquSLombMeOHbz//vucOnWKJk2ayB2OUAZJkvj555/58MMPGT58OP/85z9p2LDhIz33URokVZeMjAx2795NaGgo+/fvx9fXl+HDhzNixAieffbZx7qWwWBAqRSVyj8TSbcW++yzzzh8+DCHDx9GpRLVIFMSHx/P9OnTSU9PZ926dfj7+8sdUqWo1WoOHz5MSEgIoaGhODo6MmLECIYPH46/vz8WFhblPjc8PJzNmzcTHx/PP/7xD4YOHSrmD1Nx0kWSpHL/+Pn5SYK89Hq9FBoaKqnVarlDEf4nPz9f+vTTTyVnZ2fpyy+/lLRardwhVRm9Xi+dPn1a+uSTTyRfX1/JxcVF+te//iUVFhaWOjc5OVlyd3eXjh8/LoWHh0uDBw+W0tLSZIja9ABnpHLyqhg6mTilUklQUJAYPZiIffv2MX36dDp27MiFCxdo0aKF3CFVKaVSSefOnencuTPz5s3j5s2baLXaUqNdrVbLL7/8wvDhw+nevTtarZYvvviCnJwcGjdubDwvKiqKsLAwBg4cSN++fWv65ZgkUYipBSpKuPn5+aSmpqLTld1URagad+7c4cUXX2TatGmsXLmSbdu2mV3CLUvLli159tlnS9Vsf//9d27cuMHAgQMBuHnzJv7+/sTGxhrPkSSJli1b0qBBA7744gt69uxJWlpajcZvikTSraXu3r3Lt99+i6+vL++++y5DhgyROySzpNPp+Oqrr+jQoQNeXl5cunSJwYMHyx2W7FJSUsjMzKRLl6I+EWlpady/fx9XV1eg6OaaQqGgRYsWzJ49m3379jFo0CA2btwoZ9gmQSTdWujSpUusXr2aEydOsGbNGn755RdsbGz47rvv5A7NrJw8eZKuXbsSEhLCsWPHmDdvnpjb+j95eXkkJibStGlT1Go1V69exdbWlrZt2wIYR8YXLlxgw4YNHD9+nAMHDmBpWbQqry4v1BA13VqmsLCQtWvXUq9ePT7//HPjR1wrKysxbaeK3L9/n48//phdu3axdOlSXnrpJVFT/wulUknTpk0BOHHiBPv27WPixIlYWlqi1+uNNWBnZ2eio6OZPXs23333HYGBgcbn11Ui6dYy7733HmlpaWzZsgUrKyuys7OJiIjAycmJF154Qe7wajVJkvj3v//NzJkzeeGFF4iNjaVBgwZyh2WS/P39ad68OY0bN6ZDhw5MnjzZWOKysLBAkiQUCgWurq588803dOvWjQsXLjB8+HDjNYrPqWtE0q1FdDodOp2OJUuWYGVlxc2bN4mOjubYsWN06dIFBwcHuUOsteLi4pg2bRpZWVmEhITQtWtXuUMyaZaWlqxcuZKlS5eSkpKCh4cHW7duJT09nbfeeosrV67g4+NjTKoXLlygXr16wB/TVHft2sXRo0cZPnw4vXr1MpYezJ1IurWISqWiYcOGvPbaawwaNIi7d+9iMBjo1q0bEydOlDu8Wkmj0TBv3jzWrVvHp59+yvTp08UilMdgbW2Nh4cHAGPGjCElJQWlUsn+/fuZNGkSvXv3pkOHDoSEhLBjxw7gj9k4HTt2JDY2ltmzZ3P9+nUGDx7MiBEjGDRoEI6OjnK9pGonVqTVQqtWrSIlJYVevXrRpEkTOnbsCBSNhFUqlfFjW139+FZMp9Nx8eJFnn32WerVq1fmctXk5GRmzZrFkiVL6sQUsJoUHx9PSEgIiYmJvP766xX2ELlz5w7/+c9/CA0NJSoqiueee47hw4czfPhw3NzcajDqqiGWAZup5ORk4xQdKLrJZmVlRWJiIk5OTiQnJ6PRaOpkw5ywsDDmzJlDhw4dSE9PZ+HChfj6+pY6r/gXlWA6cnJyiIiIICQkhPDwcNzd3Y3Lkjt06FArBhIi6ZqhvLw8vvzyS15++WWeeuopdu7cyY4dO3B0dESr1WJra0tERARXr14lLi7usZuY1GaSJDF69GjGjx/PCy+8wKpVq9i4cSO7d+823nEXagedTsfx48eN3dEKCwuNjXn69OmDlZVVlX699FwN288mE5eaTbZah6ONCu+mjoz2e7wmRCLpmqmUlBQaNGiAjY0Nr7/+OvHx8URFRQHw//7f/yM/Px8vLy9GjRpVp7b/SUpK4pNPPmHWrFm0adMGg8FA8+bNee+995g1a1aFDVwE0yVJEleuXDE25omLiyMwMJDhw4czZMiQJ/oev5CUxerIBI5cvQdQYhPR4nabAV4uTOvjSXu3h38dkXTriICAAHr06IGDgwPXr19nxIgR9OzZ0+wTbvG80D/XbCdMmEDLli0ZNmwY586d48CBA9y4cYO9e/eW6A0g1F6pqamEhYUREhLCkSNH6NKli7EMUXxz71Fsjk6s8sbyIumaueJks3fvXsaNG0efPn1YtGgRHh4e2NjYyB1etbl37x7/+Mc/cHJy4tVXX6VVq1bGet/ly5cJDw8nOjoalUrF1q1befPNN/H19eWdd96ROXKhquXl5bF//35CQ0MJCwujWbNmxjJEp06dyl2MUZRwr1CgffQVckVbKLWuMPGKpFsHHD16lNWrV2NnZ8fdu3fZsWMH9vb2codVbXJycggKCqJ79+5YWlpy7do1AgMDefXVV0ucl5mZaWwqPmvWLEaOHFlr+94Kj0av1xMdHW0sQ+Tk5BhnQvTt2xdr66La7IWkLMZ+E02BtuQmsKn//gjN3XgUyqIylIWDMy0mf13iHFtLC7ZO9i93Z4+Kkq64bWsmOnXqxLBhwwgKCiI3N7fKbzCYmqSkJJRKJQsXLgRg27ZtHDx4EDc3N/r162eclVC/fn2ysrL46KOPiImJ4Y033pA5cqG6WVhY0KNHD3r06MHixYuN29UvWLCAsWPHGrerP1jYCrWu7F23Gw6cikP7wHK/hlqnZ01kAusmlN2nvCJ1dwG0mbG3t2fs2LE4OTnh6uqKlZUVGo2G3NxcuUOrFj4+Pmi1WiIiIgDo3bs3rVu3Zs+ePeTl5RmngUn/205HrVZz4MCBOjWLQyjy5+3qExISGDp0KFtDwjkcl1ZhDbcikgSH4++Rkat57OeKpGtG/jrfVK/X06FDB44ePSpTRNXHYDAwZswYdu/ejVarpUmTJrRv357CwkJycnLIyMhg165dKBQK3njjDb7//nvs7OzkDluQmYuLC5MmTSLonfkV3u/IivyBpK9eIvXHmahvxZR5jgLYfi75sWMQSdeM2dnZsXr1asaMGcOtW7fkDqdKKZVK+vbti06n4/vvvwegT58+HDt2jIKCAmJiYnB0dMTCwqLOrOkXHl1canaJaWF/1uD5SbSY+i2u03/AvsMgft8xD+39lFLnqXUG4lJyHvtri5qumQsMDOTDDz8kODiY48ePm8Vor3iKmI+PD0OHDmXu3Lm0bNkSa2tr6tevj0Kh4Pnnn5c7TMGEZavL32nFurmX8e/27fqRF3uEgutnsOw8rIzraB/7a4uRbh3w/vvv07ZtW1577TUqmq1i6jIzM5kyZQpxcXHGnQmGDBnCjBkz2LJlC9OnT+f9999/rDmaQt3kaPMY402FAij758bR5vE/RYmkWwcoFArWr1/P9evX+eKLL+QO57FJksQPP/yAj48PVlZWuLu7l5h3+dJLL7F27VpiYmJK9GsVhPJ4N3XEWlU6/RnUuRTcOIukK0Qy6Mm9fBhN0iVsW3Yqda6NSol3s8dvpyrKC3WEra0tO3fupFu3brRr146hQ4fKHdIjiY2N5a233iI/P5/du3eX27xH1G2FxzHKz5Xl++NLHZcMerKObkabmQwKJZbOrriM/ARLZ9fS5wKjOpU+/jAi6dYhrq6ubNu2jeDgYI4ePYq3t7fcIZUrPz+fefPm8e233zJ37lymTp0qeiYIVSb23EkKb52HFu1A8ceI18KuPs1e/fKhz1co4Hkvl8dqglNMlBfqmO7du7Nw4UJGjBhBVlaW3OGUKSwsDB8fH27dukVMTAzTp08XCVeoEmq1mg8//JCxY8fy4eB22FpV7hOSjcqCaQGelXquGOnWQa+//jrnz59n/PjxhIaGmkxCu337Nu+99x6XL1/m22+/pX///nKHJJiRc+fO8fLLL+Pj40NMTAyNGjXCqdK9F7zLXQL8MGKkW0ctX76c/Px8PvnkE7lDQavVsnTpUjp16kTHjh2JiYkRCVeoMjqdjnnz5jFo0CDmzJnDL7/8QqNGjQCY4O/BnCGtsbW04GG90RWKop4LD2t28zBipFtHWVpasm3bNrp06YKvry/jxo2TJY7jx48zdepUmjdvTnR0NJ6elfvIJghliYuL45VXXqFBgwacO3euxE4rxSb4e+Dr6sSayAQOx99DQdHCh2LF/XSf93JhWoBnpUe4xUTSrcMaNWrErl276N+/P15eXnTqVHpaTHXJyMhg9uzZ7N27l+XLlzN69OhasQ2LUDsYDAZWrlzJ/Pnz+fzzz5k6dWqF31++rk6sm9CZjFwN288lE5eSQ7Zai6ONJd7NHBjV6fF2jqiISLp1XPv27VmzZg0vvPACp0+frvYG3waDgR9++IGPPvqIsWPHEhsba9Y7vwo179atW0yaNAmNRsOJEyce69OTs701U3q3qsboRE1XAEaPHs0rr7zCqFGjKCwsrLavc+nSJfr06cPatWvZs2cPX331lUi4QpWRJImNGzfSuXNnAgMDOXr0qEmWq0TSFQCMOzC89957VX7tvLw8Zs+ezfPPP89LL73EiRMnarSUIZi/tLQ0goODWbFiBQcPHmT27NkmMyvnr6q0vFBVO2kKNU+pVLJ582b8/f1Zt24dU6dONT72JO9raGgo77zzDr169eLixYtiN16hyu3YsYPp06fz+uuvs23bNpNv4F8l2/VU9U6agnyuXbtGz5492b59O44e7Sr9vt66dYt3332XuLg41q5dS9++fWvyZQh1QFZWFu+88w7R0dFs2rSJ5557Tu6QjCrarueJywuboxMZ+000+6+kodEZSvWoVP/vWERsGmO/iWZzdOKTfkmhGj3zzDNs2rSJsZ+s5sX1/33s91Wr1bJ48WL8/Pzo0qULMTExIuEKVW7//v34+vpSv359zp8/b1IJ92GeqLzwODtpShIUaPUsCL8C8ESTi4Xqda++F7bdx6PRPbwN5J/f14SEBLYvmoGbmxsnT56kVavqvQss1D3F9wdCQ0PZsGEDAwYMkDukx1bppHshKYsF4XGlEm722f+Qd/EghfcSqde6D42C/lbi8QKtgQXhcfi6Oj3xJGOh6hW/r7q/fAjSF+SQEf4V6sTfUNo60qDPROq1CTA+XqA1sPFCNjNm/oMZr4wUc26FKnfixAkmTpyIv78/MTExODnVzvxR6fLC6siEMnfSVNk7U7/7GOx9y/8NVLyTZm2zf/9+fHx8eOaZZ1i0aFGpx69du0avXr2wtbVl2bJlMkT45Mp7XzMj1qKwsMT1nc00GvYhGRFrKLxXcgsgpcqaeIunRMIVqpRWq+Xjjz/mhRdeYNGiRWzatKnWJlyoZNJNz9Vw5Oq9MnfStPPqjt2zz6G0LX/+5ZPspCmnadOmERYWxpUrV9i6dStXrlwp8biTkxPLly/n/fffr5U7NJT3vhoK1eTH/xen3hNQWtli49YGO89u5F0+XOI8idr5vgqm468/N5IkkZ6ezrVr17hw4QIjR46UKbKqU6mku/3s4++A+VeV3UlTLsU1yqeffhqVSsXYsWMJCQkpcY6LiwtdunTBxsamVo72yntfdZl3UCiVWDZsYTxm2bgl2nulN7usbe+rIK8HDx6wZMkSDhw4QHZ2NgqFokTiVSgUuLi48Msvv9CkSRMZI606larpVrST5qNS6wz8+J9DxO1cZTxWnKj+nLD+eqyix6rq/OIVLX/ejeDOnTu4ubkZ/+3q6kp0dHSZr62iUe7FixfZs2ePLK/3Ydf6z72GaHSlN640aAtQWJc8rrS2w1BYUOrcyu6QKtQ9O3fu5PPPP6dz585cvHiRL774gv3795casKhU5tWtoFKvpqKdNB+HpZ0j7k7uwB+J6s8J66/HKnqsKs8va5RaViKtzGhWo9GQnp5e46/3Ua6V5tANrEonXaWlLZKmZIKVNPkorWzLfI2V2SFVqFvu3LlDREQEX375JQEBAUiSRO/evYmMjCQgIEDu8KpVpZLuY+2kWYEOPl78bcyYKrlWdXN1dSU5+Y+PzcnJyTRv3rzMcx+y4ITOncucMy27GVt/Y9f5u6WOqxq2QDLo0WbeMZYYCn+/iaXLU2VepzI7pAp1S7NmzZg0aRJdu3ZFr9ejVqtp0aIFTz1V9veUOalUTbe8nTShaGM3SVcIBj1IBuOumn9V2Z005dKlSxeuXbvGrVu30Ol0bNmypdydZ5VKZYndamuL8t5XpZUNdl7PkXXs3xgK1aiTY8lPOEm9Ns+XOlfSajgS+hOLFy/m/PnzGAxPVoYSzI8kSSiVSrp06QKAhYUF9erV4969ezx48EDm6KpfpTLDKL/yd8B8cHwLt5eOJDt6O3mXD3N76UgeHN9S6rzK7qQpF6VSyapVqwgMDKR169aMHj0aHx8f1q9fz/r16wH4/fffcXd3Z8mSJXz++ee4u7uTk1N76psVva8NB05D0hWSvHI86aFLcB44DasyRrrWNjZ8+H+9SEpKYsyYMTRr1owJEyawadMmUlJSqjN8wUQdOXKEmTNnsmHDBgoLC8u8n5Camsrt27fp0KEDWq2WgwcPyhVutat074XJP55h/5W0MqeNPfSLKiDQpwnrJpjmx+wn8dc6aW0b8Vb1+3rz5k32799PREQEBw8exN3dnYEDBzJw4EB69uyJrW3ZdWHBPBw+fJgJEybw+eef89NPP+Hv78+gQYPo1asXkiQZE+/t27f5xz/+wcSJE/nb3/5GQEBArZ3rDhX3Xqh00r2QlMXYb6Ip0JYuHTyMraUFWyf7ixVpJqg631edTseZM2eIiIhg3759xMTE0L17d2MSbtu2ba2caieUb9WqVcaluwkJCezevZurV6/y6aef0rRpUwwGA0qlkgsXLtCxY0f8/PxYsGABAwcOlDv0J1ItDW/auzkxZ4g3tpaPd4kn3UlTqF7V+b6qVCr8/f357LPPOH78OElJSUydOpWEhARGjBhBixYtePXVV/npp5/4/fffn/SlCCbAzc2N3bt3o9Fo8PT0JCAgAEdHR7Zu3QoUle0MBgO5ubnMnDmTqKioWp9wH+aJWzsWNb2JQ63TV/iRVKEo2it+zhBv0eymFtgcnci8sFjUWj2KCkokVfm+Xr9+3TgKjoyM5OmnnzaOgnv06IG1tejJXNtkZGQwd+5cfHx8eOuttzAYDPz0009cvnyZhQsX8p///AeNRsOoUaPkDrVKVUt54c9ikrNqbCdNoeaMfPNvZDbpTIqiYY2/r1qtlpMnTxIREUFERASxsbH06tXLmIS9vb1FKaIWkCSJkJAQQkJCmDRpEr179+batWu8/PLLHD16lNOnT9OxY0fs7ErPD6/Nqj3pFquJnTSFmnHnzh3atWvH9evXMVjayf6+ZmZmcvDgQeNIWJIkYwLu378/zs7Oj33N7OxssUdbNSmeKqhUKikoKGDTpk188803/Pjjj4SHhxMVFcWPP/6Ivb29zJFWjxpLuoL5mDVrFoWFhaxYsULuUEqRJImrV68aR8FHjhzB29vbmIT9/f0fumXLsmXL2Lp1K7m5uSxdupQhQ4bUUPTmLzY2ljlz5vDzzz9jY2NjPL5kyRISEhK4evUqP/zwA+7u7jJGWb0qSrpIklTuHz8/P0moex48eCA1bNhQSkxMlDuUR6JWq6XDhw9Lf//73yU/Pz/J0dFRGjZsmPT1119LBoOh1Pl79uyRWrduLRkMBmn//v3S0KFDJb1eX+IcjUZTU+GbDb1eLy1btkxq1KiRtG7dujL/3xcWFsoQWc0Dzkjl5NXaNYlUqBHr168nMDCw1izJtLa2JiAggH/+85+cOXOGhIQEXnrpJQoKCkotTsnOzubw4cNMnToVhUJB06ZNsbKyKjFbwmAwsGXLFpo3b86bb77Jtm3byM/Pr+mXVaskJibSt29fdu7cSXR0NFOmTCmz5v7nJlJ1lUi6QgmFhYV89dVXzJw5U+5QKs3FxYWxY8fy3nvv4eBQcql5YmIimZmZ9OnTB4D8/Hzc3d1JTEw0npOfn09iYiIjR46kX79+7N27l7CwsJp8CbWGJEls2LCBLl26MHToUCIjI8U2TQ9hXj3ThCe2ZcsWvLy86Nixo9yhVJiEtrUAAB19SURBVIm/jrbS0tJQq9X4+PgY/52Xl0fLli2N52RkZBATE8PcuXNp27YtY8eOrdGYa4vU1FTeeOMN7ty5w+HDh2nbtq3cIdUKYqQrGEmSxNKlS2v1KPdhsrKy+P3337G0tCQjI4Nr167RtGnTEg2yHzx4wNmzZ5k8eTLLly9HrVbLGLFp2rZtGx06dKBjx46cPHlSJNzHIEa6gtG+fftQKBRmvSKoUaNGNGjQAK1WS2hoKCdPnuTvf/87gHFJatu2bbl27RqnTp3i22+/JSoqiv79+8scuWm4f/8+b7/9NmfOnCEkJIRu3brJHVKtI0a6gtHixYv58MMPzXrRQa9evXB3d8fDw4OQkBDef/99nn76afR6PUql0vhfpVJJ9+7dadmyJevWrQP+aGak1WrrZMe0ffv24evri7OzM7/99ptIuJUkRroCAGfPnuXatWtmX79UqVQsXryYxYsXk5WVhZOTE7t27eLGjRu89957nD59GisrKzp16gTA8ePHGTx4MPBHfVir1RqTz8CBAwkMDKRPnz5mO9G/uC9CeHg433//Pf369ZM7pFpNJF0BKJq4PmPGjDo1pad4G+/g4GBjm8GMjAzmz5+PVqulRYsW2NnZMWHChBLPs7OzIy0tjfPnzxMREcGyZcsYO3YsnTt3Ni7Q6NixY61r61mW48ePM3HiRHr27ElMTAz169eXO6RaT6xIE7h58yadO3fm5s2bYlns/6SmpnLx4kV69+79SI12cnNzOXLkiHGVXHp6OgMGDGDgwIEMGDCAFi1aPPQapkSj0fDZZ5+xadMm1q5dS3BwsNwh1SpiGbBQoXfffRc7OzsWLVokdyhm4/bt28bm7QcOHKBZs2bGUXDv3r1rpMFLeq6G7WeTiUvNJlutw9FGhXdTR0b7Vdwz4/z587zyyit4enqybt06GjduXO2xmhuRdIVyZWRk8Mwzz3Dp0qVyN9oUnoxer+fs2bPGUXDxTajAwEAGDhyIr69vld68vJCUxerIBI5cvQeApozucAFeLkzr40l7tz+6w+l0OhYvXsyXX37JsmXLePnll836pmp1EklXKNf8+fO5ceMG3333ndyh1BnZ2dlERkYaO6bl5OSUKEU0bdq00teubH/rq1evMnHiROrVq8d3331n1s1oaoJIukKZ1Go1Hh4eHDp0yLhCS6h5N27cMJYiDh06xFNPPVViH7k/d+qqSFHCvUKB9tF3YLa1VNLD7nd2Lf2AuXPnMm3aNLO4ASg3kXSFMm3cuJEdO3aIvgImRKfTcfr0afbt20dERAQXL16kR48exiTcpk2bMj/yP8nedugLWRXciqDuvlXwCgQQSVcogyRJqNVqMjIycHUtf+t1QV5ZWVkcOnTIWIooLCws0bzdxcUFKH8X5/T/LEWdeAGDVo1FvQY4+v8fDu0DS5xjzrtzy0UkXaFcxfNTBdMnSVKpfeQ8PT3pPXAoYcpulFVVKLx3C8sGzVGoLNFmJJH6099pPHou1k09S5xnrVLy39l9xQ4vVaRadgMWzINIuLWHQqHA09OTadOmERISQnp6OitWrCBR0RStVlvmc6xcnkKhKl7wokCBAt390kuYFcD2c8nVF7xgJFakCUItZWlpSa9evdhx157fzt8t97yMfWvIu3gQSafBqkkrbFuVHoCpdQbiUnLKeLZQ1UTSrSNEGcF8Zat1FT7uHDiNhgOmoLkTh/r2RRQWZS/1zlaXPVoWqpYoL5i55OSij4zFCbeiGr5QOznaPHzspFBaYOPWBn1OOjm/hZdznbrTd0NOIumascjISKZMmcLq1as5efIkhYWFIvmaIe+mjlirHvFH2WAos6Zro1Li3cyhjCcIVU0kXTM2b948NBoNmZmZ/Pzzz6xcuZKDBw+i0+mQJIlff/1V7hCFKjDKr+wpf/q8LPJij2AoLEAy6Cm4cZa8K0eweap9qXMlYFQnMXWwJoiarpkqLCxk3Lhxxj6vx44d48KFC4SEhBAfH09ISAjW1taMHDlS7lCFJ9TI3po+z7qUnqerUJDz2x4y9q0ByYCqfmMa9HsTu2f9SzxfoYDnvVzEdLEaIubpmjGNRoMkScZlpPfv3+fUqVOcPHmS5cuXc/LkSby8vGSOUqgKT7IizdbSgq2T/fF1dXr4ycIjEfN06yhra2tjwpUkiQYNGhAYGIi1tTWtW7cWCdeMtHdzYs4Qb2wtH+9H2tZSyZwh3iLh1iBRXjBTf54i9te/9+vXj2HDhskZnlANJvh7AFSqy5hQc0TSNUPFSfbBgwfUr18fhUJhPGYwGOjcWayxN1cT/D3wdXViTWQCh+PvoaBo4YORrhBrGxue93JhWoCnGOHKQCRdMxMWFsZPP/2EUqmkWbNmNG/enMGDB+Pt7Q3Av//9bwYMGECzZs1kjlSoLr6uTqyb0JmMXA3bzyUTl5JDtlqLg7UFW75ezu5vF9Cm1VNyh1lniaRrZqZMmcIPP/xATk4OarWay5cvs2rVKoKDg+nRoweFhYUi4dYRzvbWTOndqsSxlJ1aTh87TJtWr8oSkyCSrlm5fPky7dq1o3///gAYDAZu3LjB0aNHWbRoEWvWrOGNN96QOUpBToGBgezdu5dXX31V7lDqLDF7wYx4enpiaWnJmDFjuHjxIkqlEk9PT1577TUGDhzIhg0b5A5RkFlgYCD79+9Hr69Es3OhSoika0asra3ZuXMnbdq0YdmyZcydO5fIyEgALl26hIWFhbwBCrJr0aIFLVq0QMy/l49IumZGpVLxwQcfMH78eKysrFi0aBG+vr5YWFjw0UcfyR2eYAIGDRrE3r175Q6jzhIr0sxYYWEhVlZWJCUl0bRpUywtRRcpAQ4ePMgnn3zCiRMn5A7FbIkVaXWMTqcjPz/fmGTd3NxEwhWMevbsyeXLl8nMzJQ7lDpJJF0ztGLFCubNmyealgtlsra2pk+fPhw4cEDuUOokkXTNTGFhIV999RVjxoyROxTBhIm6rnxE0jUzW7Zswdvbmw4dOsgdimDCiufrimb2NU8kXTMiSRJLlixh5syZcocimDhPT0/s7Oy4ePGi3KHUOSLpmpG9e/diYWHBgAED5A5FqAUGDRrEvn375A6jzhFJ14wsWbKEDz/8UNxAEx6JqOvKQyRdM3H27FkSEhLEDTThkQUEBHDq1Clyc3PlDqVOEUnXTCxZsoQZM2aI+bjCI7O3t6dr164cOXJE7lDqFNFlzAzcvHmTAwcO8M0338gdilDLbNy4UbT6rGEi6ZqBL7/8kjfeeAMHBwe5QxFqGXd3d7lDqHNE0q3lMjIy2Lx5M5cuXZI7FEEQHoGo6dZya9euJTg4mObNm8sdiiAIj0CMdGsxtVrNqlWrOHTokNyhCGZCp9ORk5ODXq/H2toaKysrrK2t5Q7LrIikW4tt2rSJzp074+PjI3coghn47bffiIyM5Pbt2yQmJnLu3DlGjRrFjBkzcHNzkzs8syHKC7WUwWBg2bJlYsmv8MS0Wi3vvPMOkydP5u7du/j7+zN//nxu3bpFfn4+X331ldwhmhUx0q2lQkNDqV+/Pr1795Y7FKGWO3nyJOnp6ezduxdnZ+cSj/n6+hIfHy9TZOZJJN1aqrixjVjyKzwptVpNXFwczs7O5OTkkJWVRXJyMlFRUfz666+sWLFC7hDNiki6Jiw9V8P2s8nEpWaTrdbhaKPCu6kjHoYUUlNTGTlypNwhCmagf//+PP300/Tu3ZvnnnsOa2tr0tPTsbe3Z+PGjXh7e8sdolkRSdcEXUjKYnVkAkeu3gNAozMYH7NRpaIpLMTzlYVcuptDezcnucIUzMiGDRuIi4sjKSmJ+/fvM2DAAHx9falfv76x5674VFU1RNI1MZujE1kQHodap6es/tJqnQGUKhLUKsZ+E82cId5M8Peo8TgF8yJJElZWVowePbrUYyLZVi0xe8GEFCXcKxRoy064fyYBBVo9C8KvsDk6sSbCE8xcREQEUJSAi/9A0UyZxMRE8vPz5QzPbIiRrom4kJTFgvA4CrR/lBIknZaMiDWoE89jUOeicmpGgz6vYNvqj52dC7QGFoTH4evqhK+rKDUIldOgQQOmTJkClB7ZKpVKtm/fjoODg/EcofLESNdErI5MQK3TlzgmGfSoHBrR9KVFuP1tK069J3Av5At0WWklzlPr9KyJTKjJcAUzlJGRQWhoKAZD0S/+tLQ0oqKiAPD29iYsLEzO8MyGSLomID1Xw5Gr90qVFJRWNjj1Go/KqQkKhRI7z66o6jdBk1oywUoSHI6/R0aupgajFsxNamoq69evR6ksSgtKpZK//e1vAPTr14/r16+j0+nkDNEsiPKCCdh+NvmRztPn3UebeQcrl9Lt+BTA9nPJTOndqoqjE+qKbt26kZycTGFhIVZWVjRq1Ijr16+zdOlSUlJSGDlypNg9uAqIpGsC4lKzS0wLK4uk15EeuhT7dv2wdC69Dl6tMxCXklNdIQp1gKWlJR4eHnz11Ve4u7uze/du5s+fj06n49atW3z++ediZ5IqIMoLJiBbXfFHNkkykB62DCxUNBwwtYLraKs6NKGOWblyJdnZ2SxevBhvb2/Gjx/Pu+++y/bt20VjpSoiRromwNGm/LdBkiQywv+FPi+LxqPnorAo/1xHGzEKEZ6Mm5sb8+bNY968eSWOGwwGFAqFmLNbBcRI1wR4N3XEWlX2W5G5bzXajCQaj/oMpWX5fU1tVEq8m4nteoSqYTAYStRvlUqlSLhVRIx0TcAoP1e+PHC11HHdg9/JPb8XLCxJXvmy8XjDQdOxb/N8iXMlYFQn1+oOVagjDAaDsZG5ULVE0jUBjeyt6fOsC/uvpJWYNqaq35inPnr43EiFAp73csHZXvyACFUjMTGRFStWsGrVKrlDMTuivGAipgd4YqOyqNRzrS2UTAvwrOKIhLqsWbNm/PDDD+TkiBkxVU0kXRPR3s2JOUO8sbV8vLdEhR5N9M/U02RUU2RCXVSvXj38/f05fPiw3KGYHZF0TcgEfw/mDGmNraUFD7tnoVCAraUFc0f48tGoHvTu3ZuYmJiaCVSoEwIDA9m7d6/cYZgdUdM1MRP8PfB1dWJNZAKH4++h4H/tHP/HRqVEoqiGOy3As6jJjf8UnJyc6N+/Pzt37qRHjx6yxS+Yj0GDBjF8+HAkSRIzF6qQSLomyNfViXUTOpORq2H7uWTiUnLIVmtxtLHEu5kDozq5lrppNmbMGJycnAgODmbTpk0MHjxYpugFc9GmTRsKCwtJSEjgmWeekTscsyGSrglztrd+rF4KgYGBhIaGEhwczIoVKxg3blw1RieYO4VCwaBBg9i7d69IulVI1HTNzHPPPcfBgweZOXMma9askTscoZYTdd2qJ5KuGWrbti3Hjh1j+fLlfP7556IzlFBp/fv359ixY6jVarlDMRsi6Zqpli1bEhUVxY4dO5gxY4axMbUgPI4GDRrQrl07YzNz4cmJpGvGmjZtypEjRzh79iwTJ05EqxVdyITHV1zXFaqGSLpmzsnJiYiICDIzM/m///s/CgoK5A5JqGVEXbdqiaRbB9jZ2bFr1y4cHBwIDAzkwYMHcock1CJ+fn6kpqaSlJQkdyhmQSTdOsLS0pIff/yR9u3bExAQQFpa2sOfJAiAhYUFAwcONG7RLjwZkXTrEKVSyb/+9S9GjBhBr169SExMlDskoZYQJYaqI5JuHaNQKJg7dy5vv/02vXr1IjY2Vu6QhFpg4MCBHDhwQOwGXAXEirQ66t1338XZ2Zm+ffsSEhJCt27d5A5JMGHNmjXD/dk2fPLvI6htGpKt1uFoo8K7qSOj/UovSxfKp6ho4nznzp2lM2fO1GA4Qk3bvXs3CxcuJDIyEpVK/A4WSruQlMXqyAQOxKagQELPH32fixswBXi5MK2PJ+3dnOQL1IQoFIqzkiR1LvMxkXQFrVaLhYUFSmXZ1abk5GQMBgPu7u41HJkgt83RiSwIj0Ot01PRwkaFAmxUFswZ4s0Ef48ai89UVZR0xdBGwNKy7F2E79y5w9KlSzl16hQ5OTmMHz+e2bNn13B0glyKEu4VCrQPX80oSVCg1bMg/AqASLwVEDfShDLFx8ezatUqHjx4wO7duzl+/Djh4eFcvVp6A03B/FxIymJBeNwjJdw/K9AaWBAeR0xyVjVFVvuJka5QSnZ2Nj///DOFhYV88cUXODk5cfXqVbKzs7Gzs5M7PKEGrI5MQK3Tlzh2e9moEv+WdIU4dBxCw4FTSxxX6/SsiUxg3YQyP13XeSLpCqWEhIRw6tQp1q5di4uLC5mZmRw9epQRI0bg6uoqdhIwc+m5Go5cvVeqhuv+wXbj3w2FapJXTsDOu2ep50sSHI6/R0auRsxqKIMoLwgl6HQ6wsPDefvtt3nqqae4f/8+hw8f5syZM7Rt2xZAJFwzt/1s8kPPyY8/joVdfazd2pT5uALYfu7h16mLRNIVSlAqldjZ2XHy5ElycnL4+uuvOXLkCB07dmTUqD8+XiYlJXHy5EnWr1+PXq+v4IpCbROXmo1GV3EtN/fiQeq17VvuL2C1zkBciti+vSyivCCUoFQqWbx4MS+++CJRUVG0b9+eoUOHEhgYCBSNhCVJYv369aSmppKRkcHWrVv59ddfqV+/vszRC1UhW13xqjPdg9/RJF3Ceci7D7mOaCVaFpF0hVKcnZ3Zv38/WVlZ2NvbY2VlVeJxlUpFbm4uQ4cOJTg4mDlz5rBixQo+++wzUXowA442FaeF3EuHsHb1wdKp6UOuU/ZUxLpOJF2hTEqlkoYNGxIeHk5KSgqvv/46GRkZLFq0CEmSOHLkCGlpaQQHB3Pnzh0cHR1Fwq3FDAYDv/32G2FhYUTEZCI93QeFquybYHmXDlHff1SZjxWzUSnxbuZQHaHWeqKmK1RoyJAhdOjQAYBvvvmGgoICJk2axLlz54iNjWXChAnExsYyd+5ceQMVHlteXh4hISG8+eabuLq6Mn78eHJyclg0ORhrG9syn6NOvoI+N6PMWQt/JgGjOrlWQ9S1nxjpCg/l5+cHQGFhIa1ataJNm6I71sOGDeOZZ57h+++/R6VSYTAYyl1KLJiGW7duERYWRlhYGMePH6dr164EBQUxa9asEtush/1+hv1X0kpNG8u7dBC7Z7ujtC5/vrZCAc97uYjpYuUQSVd4ZC4uLvz444+8+eabHDp0iEOHDhEUFGRslCMSrunR6/WcPHnSmGhTU1MZMmQIr7/+Olu2bCn35uf0AE+OXUunQFtyZorzoLcf+jVtVBZMC/CskvjNkUi6wiN76623yMvL44UXXiA1NZU5c+bQsWNHucMS/iIrK4t9+/axe/du9uzZQ4sWLQgKCmL9+vV06dIFCwuLh16jvZsTc4Z4P3LvhWK2lkrmDPHG11V0GyuP6DImPLZ79+5hYWFBw4YNjce0Wm25jXOE6hcfH28czZ49e5bevXsTFBTE0KFDcXNzq/R1RZexyhFdxoQq5eLiUuLfBoOBFStWkJWVxfz588UshhpQWFhIVFSUMdHm5+cTFBTEBx98QN++fausR8YEfw98XZ1YE5nA4fh7KCha+FCsuJ/u814uTAvwFCPcRyBGukKVuHfvHoMHD6Zz586sXr36kT7C1nXnzp3jzJkzTJ48+ZHOv3fvHnv27CEsLIz9+/fj5eVFUFAQQUFBtG/fvtp/2WXkath+Lpm4lByy1VocbSzxbubAqE5i54i/Ek3MhRqRnZ1NcHCw8YbbXxdVCEUyMzMZOXIkGo2GLl26sHDhQurVq1fqPEmSiImJYffu3YSFhREbG0v//v0JCgpi8ODBNGnSRIbohUchkq5QY9RqNePGjSM/P59ff/21zGRS1y1cuBCdTsenn35a7jlarRYfHx8MBgPDhg0jKCiIXr16YW0tRpS1QUVJV8zxEaqUjY0N27Zto0WLFvTv35/MzEy5Q6pxly9fZvLkycyaNYtLly4ZjxsMRbXQgoIC3Nzc0Gg0hISEkJSUVOoaBoOBQ4cOkZCQwIoVK+jfv79IuGZCJF2hyqlUKjZs2ECPHj3o06cPd+/elTukGpOdnc2yZcto3LgxLi4ufPDBByQkJABF85hzc3NRq9Vcu3aNQYMGsWXLFl5++WV+++23EtextrbGzc1N3JQ0Q2L2glAtFAoFS5YsoVGjRvTs2ZOIiAg8Pc1/wnxKSgrHjx8nPj4eKOrKtmHDBubPn4+FhQX29vY4OjqyY8cO47zZjz/+mB07dtC8eXNRp60DxEhXqDYKhYKPPvqIjz76iD59+nDhwgW5Q6p2VlZWeHl5kZKSAoC/vz8ZGRklygxdu3aldevW3L9/H4D+/ftz+vTpEvOeBfMlkq5Q7SZPnsyKFSsYMGAAUVFRcodTrYrLAufPnwfAzc0NZ2fnUkm3X79+fPvtt1y7do3w8HD8/PzE4pI6QiRdoUaMHj2azZs388ILLxAeHi53OFUmOzsbrfaPZt3Ozs60aNGCEydOANCwYUNycnJo1KgRUHSDzMnJiddee41u3boxbdo09Ho9b7/98J4GgnkQU8aEGhUdHU1wcDDLli1j/Pjxcofz2CRJ4sqVK8aVYBcvXuTu3bvY2v7RCvHEiRO8/fbbHD9+HBsbG/z9/dmwYQMZGRn897//5dVXX6Vp06bo9XqxiMRMiWXAgsnw9/fn4MGDDBo0iPv379eKEZ5Go+HIkSPGRKvX6xk2bBgff/wxAQEB2NjYlDj/ueeeo0uXLkycOJHz58/TrVs3PDw8cHd3p0ePHsZEKxJu3SSSrlDj2rRpw7FjxxgwYAAZGRkmuc1Pamoq4eHhhIWFcfDgQdq1a0dQUBChoaG0adPmofGuXLmS06dPY2FhQbdu3WooaqE2EElXkIWHhwdRUVEEBgaSkZHBihUrSvXjTc/VsP1sMnGp2WSrdTjaqPBu6shov6pf6y9JknG7mrCwMK5du0ZgYCAjR45k/fr1xprso7K0tKR79+5VGqNgHkRNV5BVVlYWw4YN46mnnmLjxo1YWlpyISmL1ZEJHLl6D6DEduDFXa0CvFyY1seT9m6V72qVl5fHwYMHCQsLY/fu3Tg4OBgbyPTo0UPMJhAqTfReEExafn4+L774IgAjZy1nyYEb1da/9datW8YGMlFRUcbtaoYOHVpiuxpBeBIi6QomT6vVMvidBVx37IBk8egjzKKdClqXm3jL264mKCiIAQMGlLtdjSA8CTF7QTB5sal5JDfuhlTO1jDazDvc3fA29bx70GjYh8bjBVoDC8Lj8HV1MjbQzsrKIiIigrCwMPbs2UPz5s0JCgri66+/pmvXrmLWgCArkXQFk7A6MqFE7favMiPWYd2s7I//ap2eL8LO45d/jrCwMM6cOUPv3r0ZOnQo8+fPx93dvbrCFoTHJpKuILv0XA1Hrt4rt4abF3sEpU09LJ290WWllHpckiDqehbWuYm8//77VbpdjSBUNbEMWJDd9rPJ5T5m0OSTdezfNOj7eoXXsLWxptu49wgKChIJVzBpIukKsotLzS63tJB19Efs2w9E5ehS5uPF1DoDcSk51RGeIFQpkXQF2WWrdWUeL0y7gfrWBRy7jHjE62gffpIgyEzUdAXZOdqU/W2ovn0R3YM0ktdMAkAqVINkICX9PZpN+qqM64jFDILpE0lXkJ13U0esVamlSgz2HQKp17q38d/Zp35F9yCNhoHTS13DRqXEu5lDtccqCE9KlBcE2Y3ycy3zuNLSBgv7BsY/CksbFCorLOxKL2iQgFGdyr6OIJgSMdIVZNfI3po+z7qw/0pahUt/nXqV3X9XoYDnvVyqvAmOIFQHMdIVTML0AE9sVJVbKWajsmBagPlveimYB5F0BZPQ3s2JOUO8sbV8vG/Jot4L3sYlwIJg6kR5QTAZxU1rFoTHVVuXMUGQm0i6gkmZ4O+Br6sTayITOBx/DwVFCx+KFffTfd7LhWkBnmKEK9Q6IukKJsfX1Yl1EzqTkath+7lk4lJyyFZrcbSxxLuZA6M6Vf3OEYJQUyrsp6tQKO4Bt2ouHEEQBLPwlCRJZa5drzDpCoIgCFVLzF4QBEGoQSLpCoIg1CCRdAVBEGqQSLqCIAg1SCRdQRCEGvT/AfW6ndlvBcQFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Goal node is to always get to node 7 the fastest\n",
    "goal = 7\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edge_list)\n",
    "\n",
    "# Add weights to edges\n",
    "for i, tup in enumerate(edge_list):\n",
    "    source = tup[0]\n",
    "    sink = tup[1]\n",
    "    G[source][sink]['weight'] = weights[i]\n",
    "        \n",
    "# position = nx.spring_layout(G)\n",
    "position=nx.nx_agraph.graphviz_layout(G)\n",
    "\n",
    "nx.draw_networkx_nodes(G, position)\n",
    "nx.draw_networkx_edges(G, position)\n",
    "nx.draw_networkx_labels(G, position)\n",
    "\n",
    "labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, position, edge_labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_MATRIX = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up immediate rewards matrix\n",
    "R = np.matrix(np.ones(shape=(SIZE_MATRIX, SIZE_MATRIX)))\n",
    "\n",
    "# Set initial rewards to -1\n",
    "R *= -1\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2)\n",
      "(0, 1)\n",
      "(0, 3)\n",
      "(2, 4)\n",
      "(5, 6)\n",
      "(7, 4)\n",
      "(0, 6)\n",
      "(5, 3)\n",
      "(3, 7)\n",
      "(0, 8)\n"
     ]
    }
   ],
   "source": [
    "# Set the reward for any edge which leads to the goal of 7 to 100\n",
    "for edge in edge_list:\n",
    "    print(edge)\n",
    "    # If edge leads to 7, reward is 100\n",
    "    if edge[1] == goal:\n",
    "        R[edge] = 100\n",
    "    else:\n",
    "        R[edge] = 0\n",
    "    # Undirected so if we're at 7 then set 100 as reward\n",
    "    if edge[0] == goal:\n",
    "        R[edge[::-1]] = 100\n",
    "    else:\n",
    "        R[edge[::-1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ -1.,   0.,   0.,   0.,  -1.,  -1.,   0.,  -1.,   0.],\n",
       "        [  0.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.],\n",
       "        [  0.,  -1.,  -1.,  -1.,   0.,  -1.,  -1.,  -1.,  -1.],\n",
       "        [  0.,  -1.,  -1.,  -1.,  -1.,   0.,  -1., 100.,  -1.],\n",
       "        [ -1.,  -1.,   0.,  -1.,  -1.,  -1.,  -1., 100.,  -1.],\n",
       "        [ -1.,  -1.,  -1.,   0.,  -1.,  -1.,   0.,  -1.,  -1.],\n",
       "        [  0.,  -1.,  -1.,  -1.,  -1.,   0.,  -1.,  -1.,  -1.],\n",
       "        [ -1.,  -1.,  -1.,   0.,   0.,  -1.,  -1., 100.,  -1.],\n",
       "        [  0.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once in goal state, want to make sure we remain there\n",
    "R[goal, goal] = 100\n",
    "\n",
    "# Initialized Matrix\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discount factor\n",
    "gamma = 0.8\n",
    "\n",
    "# Set Q-Table\n",
    "Q = np.matrix(np.zeros(shape=[SIZE_MATRIX, SIZE_MATRIX]))\n",
    "pd.DataFrame(Q)\n",
    "# x = pd.DataFrame(Q)\n",
    "# x.iloc[6,3]= 3\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row 7 of rewards matrix has 3 node that we can access; 3,4,7.\n",
    "# Thus nodes 3, 4, and 7 are states we can explore\n",
    "# np.where(R[7,:] >= 0)[1]\n",
    "# R[7,]\n",
    "\n",
    "def get_available_actions(state):\n",
    "    current_state_row = R[state,] # Row[7,] = matrix([[ -1.,  -1.,  -1.,   0.,   0.,  -1.,  -1., 100.,  -1.]])\n",
    "    available_actions = np.where(current_state_row >= 0)[1] # array([3, 4, 7])\n",
    "    return available_actions\n",
    "\n",
    "def sample_next_action(available_actions):\n",
    "    # Randomly choose any available action; 3,4,7.\n",
    "    next_action = int(np.random.choice(available_actions, size=1))\n",
    "    return next_action\n",
    "\n",
    "def update(current_state, action, gamma):\n",
    "    \n",
    "    # Find index of action which has highest Q-Value\n",
    "    max_index = np.where(Q[action,]==np.max(Q[action,]))[1]\n",
    "    # Which nodes can this exploratory node even get to?\n",
    "#     max_index = get_available_actions(action)\n",
    "    print(f'Max Actions to explore from node {action}: {max_index}')\n",
    "    \n",
    "    # If more than one index has max Q-Value, choose at random\n",
    "    if max_index.shape[0]>1:\n",
    "        max_index = int(np.random.choice(max_index, size=1))\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "    \n",
    "    print('Chosen Node/Action to explore:', max_index)\n",
    "    print(f\"Immediate Edge: {[current_state, action]}. Future Edge/State to explore for Q-Learning: {[action, max_index]}\")\n",
    "        \n",
    "    # Grab next state max action-value combination that's currently estimated for that Q-Value\n",
    "    max_value = Q[action, max_index]\n",
    "    print(f'Future Edge/State Q-Value Index {[action, max_index]} has value: {max_value}')\n",
    "    \n",
    "    # Now can populate new Q-Value for state-action combination: Immediate Reward + Discounted Reward for next state\n",
    "    print(f\"Current Reward for starting state-action pair index {[current_state, action]}: {R[current_state, action]}\")\n",
    "    print(f\"Future discounted reward for next state at Q-Table index {[action, max_index]}: {gamma * max_value}\")\n",
    "    Q[current_state, action] = R[current_state, action] + gamma * max_value\n",
    "    print(f\"Updated Q-Value at index {[current_state, action]} with value: {R[current_state, action] + gamma * max_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Node/State: 0\n",
      "Nodes to Choose from: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 5]\n",
      "Future Edge/State Q-Value Index [1, 5] has value: 0.0\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 5]: 0.0\n",
      "Updated Q-Value at index [0, 1] with value: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Lets start at Node 0\n",
    "initial_state = 0\n",
    "print(f\"Initial Node/State: {initial_state}\")\n",
    "\n",
    "available_actions = get_available_actions(initial_state)\n",
    "print(f\"Nodes to Choose from: {available_actions}\") # Can get to node 1, 2, 3, 6, or 8\n",
    "\n",
    "action = sample_next_action(available_actions)\n",
    "print(f\"Randomly selected node: {action}\") # randomly selected node 1\n",
    "\n",
    "update(initial_state, action, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 1\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 1]\n",
      "Future Edge/State Q-Value Index [4, 1] has value: 0.0\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 1]: 0.0\n",
      "Updated Q-Value at index [7, 4] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 7]\n",
      "Future Edge/State Q-Value Index [0, 7] has value: 0.0\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 7]: 0.0\n",
      "Updated Q-Value at index [1, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 0]\n",
      "Future Edge/State Q-Value Index [7, 0] has value: 0.0\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 0]: 0.0\n",
      "Updated Q-Value at index [4, 7] with value: 100.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 0]\n",
      "Future Edge/State Q-Value Index [3, 0] has value: 0.0\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 0]: 0.0\n",
      "Updated Q-Value at index [7, 3] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 6\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 6]\n",
      "Future Edge/State Q-Value Index [6, 6] has value: 0.0\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 6]: 0.0\n",
      "Updated Q-Value at index [5, 6] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 7]\n",
      "Future Edge/State Q-Value Index [6, 7] has value: 0.0\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 7]: 0.0\n",
      "Updated Q-Value at index [5, 6] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 7]\n",
      "Future Edge/State Q-Value Index [0, 7] has value: 0.0\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 7]: 0.0\n",
      "Updated Q-Value at index [2, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 100.0\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 80.0\n",
      "Updated Q-Value at index [7, 4] with value: 80.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 5]\n",
      "Future Edge/State Q-Value Index [5, 5] has value: 0.0\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 5]: 0.0\n",
      "Updated Q-Value at index [3, 5] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 1\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 1]\n",
      "Future Edge/State Q-Value Index [6, 1] has value: 0.0\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 1]: 0.0\n",
      "Updated Q-Value at index [5, 6] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 5]\n",
      "Future Edge/State Q-Value Index [5, 5] has value: 0.0\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 5]: 0.0\n",
      "Updated Q-Value at index [6, 5] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 1\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 1]\n",
      "Future Edge/State Q-Value Index [0, 1] has value: 0.0\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 1]: 0.0\n",
      "Updated Q-Value at index [1, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 0.0\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 0.0\n",
      "Updated Q-Value at index [6, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 4]\n",
      "Future Edge/State Q-Value Index [5, 4] has value: 0.0\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 4]: 0.0\n",
      "Updated Q-Value at index [6, 5] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 5]\n",
      "Future Edge/State Q-Value Index [5, 5] has value: 0.0\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 5]: 0.0\n",
      "Updated Q-Value at index [6, 5] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 100.0\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 80.0\n",
      "Updated Q-Value at index [7, 4] with value: 80.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 100.0\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 80.0\n",
      "Updated Q-Value at index [2, 4] with value: 80.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 0.0\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 0.0\n",
      "Updated Q-Value at index [8, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 6\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 6]\n",
      "Future Edge/State Q-Value Index [5, 6] has value: 0.0\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 6]: 0.0\n",
      "Updated Q-Value at index [6, 5] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 1\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 1]\n",
      "Future Edge/State Q-Value Index [0, 1] has value: 0.0\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 1]: 0.0\n",
      "Updated Q-Value at index [8, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 8\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 8]\n",
      "Future Edge/State Q-Value Index [6, 8] has value: 0.0\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 8]: 0.0\n",
      "Updated Q-Value at index [5, 6] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 7]\n",
      "Future Edge/State Q-Value Index [0, 7] has value: 0.0\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 7]: 0.0\n",
      "Updated Q-Value at index [1, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 0.0\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 0.0\n",
      "Updated Q-Value at index [1, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 80.0\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 64.0\n",
      "Updated Q-Value at index [4, 2] with value: 64.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 4]\n",
      "Future Edge/State Q-Value Index [7, 4] has value: 80.0\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 4]: 64.0\n",
      "Updated Q-Value at index [7, 7] with value: 164.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 7]\n",
      "Future Edge/State Q-Value Index [0, 7] has value: 0.0\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 7]: 0.0\n",
      "Updated Q-Value at index [8, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 164.0\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 131.20000000000002\n",
      "Updated Q-Value at index [4, 7] with value: 231.20000000000002\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 0.0\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 0.0\n",
      "Updated Q-Value at index [8, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 231.20000000000002\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 184.96000000000004\n",
      "Updated Q-Value at index [2, 4] with value: 184.96000000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 6\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 6]\n",
      "Future Edge/State Q-Value Index [0, 6] has value: 0.0\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 6]: 0.0\n",
      "Updated Q-Value at index [2, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 7]\n",
      "Future Edge/State Q-Value Index [0, 7] has value: 0.0\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 7]: 0.0\n",
      "Updated Q-Value at index [6, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 231.20000000000002\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 184.96000000000004\n",
      "Updated Q-Value at index [2, 4] with value: 184.96000000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 4]\n",
      "Future Edge/State Q-Value Index [0, 4] has value: 0.0\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 4]: 0.0\n",
      "Updated Q-Value at index [8, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 164.0\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 131.20000000000002\n",
      "Updated Q-Value at index [7, 7] with value: 231.20000000000002\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 1\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 1]\n",
      "Future Edge/State Q-Value Index [0, 1] has value: 0.0\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 1]: 0.0\n",
      "Updated Q-Value at index [6, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 231.20000000000002\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 184.96000000000004\n",
      "Updated Q-Value at index [2, 4] with value: 184.96000000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 7]\n",
      "Future Edge/State Q-Value Index [0, 7] has value: 0.0\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 7]: 0.0\n",
      "Updated Q-Value at index [1, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 1\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 1]\n",
      "Future Edge/State Q-Value Index [0, 1] has value: 0.0\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 1]: 0.0\n",
      "Updated Q-Value at index [6, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 0.0\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 0.0\n",
      "Updated Q-Value at index [5, 6] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 0.0\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 0.0\n",
      "Updated Q-Value at index [0, 3] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 231.20000000000002\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 184.96000000000004\n",
      "Updated Q-Value at index [7, 7] with value: 284.96000000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 4]\n",
      "Future Edge/State Q-Value Index [0, 4] has value: 0.0\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 4]: 0.0\n",
      "Updated Q-Value at index [2, 0] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 184.96000000000004\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 147.96800000000005\n",
      "Updated Q-Value at index [0, 2] with value: 147.96800000000005\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [2, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 231.20000000000002\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 184.96000000000004\n",
      "Updated Q-Value at index [2, 4] with value: 184.96000000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 284.96000000000004\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 227.96800000000005\n",
      "Updated Q-Value at index [4, 7] with value: 327.9680000000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 184.96000000000004\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 147.96800000000005\n",
      "Updated Q-Value at index [0, 2] with value: 147.96800000000005\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 7]\n",
      "Future Edge/State Q-Value Index [1, 7] has value: 0.0\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 7]: 0.0\n",
      "Updated Q-Value at index [0, 1] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 0.0\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 0.0\n",
      "Updated Q-Value at index [5, 3] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 284.96000000000004\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 227.96800000000005\n",
      "Updated Q-Value at index [4, 7] with value: 327.9680000000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 4]\n",
      "Future Edge/State Q-Value Index [6, 4] has value: 0.0\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 4]: 0.0\n",
      "Updated Q-Value at index [0, 6] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 2]\n",
      "Future Edge/State Q-Value Index [3, 2] has value: 0.0\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 2]: 0.0\n",
      "Updated Q-Value at index [5, 3] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [2, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 184.96000000000004\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 147.96800000000005\n",
      "Updated Q-Value at index [4, 2] with value: 147.96800000000005\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 184.96000000000004\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 147.96800000000005\n",
      "Updated Q-Value at index [0, 2] with value: 147.96800000000005\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 4]\n",
      "Future Edge/State Q-Value Index [3, 4] has value: 0.0\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 4]: 0.0\n",
      "Updated Q-Value at index [5, 3] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 0.0\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 0.0\n",
      "Updated Q-Value at index [0, 1] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [6, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [2, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 284.96000000000004\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 227.96800000000005\n",
      "Updated Q-Value at index [4, 7] with value: 327.9680000000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 327.9680000000001\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 262.3744000000001\n",
      "Updated Q-Value at index [2, 4] with value: 262.3744000000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 5]\n",
      "Future Edge/State Q-Value Index [3, 5] has value: 0.0\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 5]: 0.0\n",
      "Updated Q-Value at index [0, 3] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [6, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [2, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 4]\n",
      "Future Edge/State Q-Value Index [5, 4] has value: 0.0\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 4]: 0.0\n",
      "Updated Q-Value at index [3, 5] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 284.96000000000004\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 227.96800000000005\n",
      "Updated Q-Value at index [7, 7] with value: 327.9680000000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [0 1 2 3 4 5 6 7 8]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 5]\n",
      "Future Edge/State Q-Value Index [3, 5] has value: 0.0\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 5]: 0.0\n",
      "Updated Q-Value at index [7, 3] with value: 0.0\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [8, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [8, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 327.9680000000001\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 262.3744000000001\n",
      "Updated Q-Value at index [4, 7] with value: 362.3744000000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 362.3744000000001\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 289.8995200000001\n",
      "Updated Q-Value at index [2, 4] with value: 289.8995200000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 327.9680000000001\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 262.3744000000001\n",
      "Updated Q-Value at index [3, 7] with value: 362.3744000000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 362.3744000000001\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 289.8995200000001\n",
      "Updated Q-Value at index [7, 4] with value: 289.8995200000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [8, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [6, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [8, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [1, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [2, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 289.8995200000001\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 231.9196160000001\n",
      "Updated Q-Value at index [4, 2] with value: 231.9196160000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [3, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [2, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 327.9680000000001\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 262.3744000000001\n",
      "Updated Q-Value at index [7, 7] with value: 362.3744000000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 362.3744000000001\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 289.8995200000001\n",
      "Updated Q-Value at index [3, 7] with value: 389.8995200000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 147.96800000000005\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 118.37440000000004\n",
      "Updated Q-Value at index [6, 0] with value: 118.37440000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 362.3744000000001\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 289.8995200000001\n",
      "Updated Q-Value at index [3, 7] with value: 389.8995200000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 289.8995200000001\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 231.9196160000001\n",
      "Updated Q-Value at index [4, 2] with value: 231.9196160000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 362.3744000000001\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 289.8995200000001\n",
      "Updated Q-Value at index [7, 4] with value: 289.8995200000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 289.8995200000001\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 231.9196160000001\n",
      "Updated Q-Value at index [0, 2] with value: 231.9196160000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 118.37440000000004\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 94.69952000000004\n",
      "Updated Q-Value at index [0, 1] with value: 94.69952000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 118.37440000000004\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 94.69952000000004\n",
      "Updated Q-Value at index [5, 6] with value: 94.69952000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 118.37440000000004\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 94.69952000000004\n",
      "Updated Q-Value at index [5, 6] with value: 94.69952000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 362.3744000000001\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 289.8995200000001\n",
      "Updated Q-Value at index [7, 7] with value: 389.8995200000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [6]\n",
      "Chosen Node/Action to explore: 6\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 6]\n",
      "Future Edge/State Q-Value Index [5, 6] has value: 94.69952000000004\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 6]: 75.75961600000004\n",
      "Updated Q-Value at index [6, 5] with value: 75.75961600000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [8, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 389.8995200000001\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 311.91961600000013\n",
      "Updated Q-Value at index [7, 3] with value: 311.91961600000013\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [8, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [2, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 118.37440000000004\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 94.69952000000004\n",
      "Updated Q-Value at index [0, 6] with value: 94.69952000000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [6, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [8, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 389.8995200000001\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 311.91961600000013\n",
      "Updated Q-Value at index [3, 7] with value: 411.91961600000013\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [8, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 411.91961600000013\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 329.5356928000001\n",
      "Updated Q-Value at index [7, 3] with value: 329.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [8, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [6]\n",
      "Chosen Node/Action to explore: 6\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 6]\n",
      "Future Edge/State Q-Value Index [5, 6] has value: 94.69952000000004\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 6]: 75.75961600000004\n",
      "Updated Q-Value at index [6, 5] with value: 75.75961600000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [1, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [6]\n",
      "Chosen Node/Action to explore: 6\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 6]\n",
      "Future Edge/State Q-Value Index [5, 6] has value: 94.69952000000004\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 6]: 75.75961600000004\n",
      "Updated Q-Value at index [3, 5] with value: 75.75961600000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [8, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 362.3744000000001\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 289.8995200000001\n",
      "Updated Q-Value at index [2, 4] with value: 289.8995200000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 362.3744000000001\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 289.8995200000001\n",
      "Updated Q-Value at index [2, 4] with value: 289.8995200000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 289.8995200000001\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 231.9196160000001\n",
      "Updated Q-Value at index [4, 2] with value: 231.9196160000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 389.8995200000001\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 311.91961600000013\n",
      "Updated Q-Value at index [7, 7] with value: 411.91961600000013\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 411.91961600000013\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 329.5356928000001\n",
      "Updated Q-Value at index [4, 7] with value: 429.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 411.91961600000013\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 329.5356928000001\n",
      "Updated Q-Value at index [7, 7] with value: 429.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 411.91961600000013\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 329.5356928000001\n",
      "Updated Q-Value at index [7, 3] with value: 329.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 411.91961600000013\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 329.5356928000001\n",
      "Updated Q-Value at index [7, 3] with value: 329.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [6]\n",
      "Chosen Node/Action to explore: 6\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 6]\n",
      "Future Edge/State Q-Value Index [5, 6] has value: 94.69952000000004\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 6]: 75.75961600000004\n",
      "Updated Q-Value at index [3, 5] with value: 75.75961600000004\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 411.91961600000013\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 329.5356928000001\n",
      "Updated Q-Value at index [5, 3] with value: 329.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 185.5356928000001\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 148.4285542400001\n",
      "Updated Q-Value at index [5, 6] with value: 148.4285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 329.5356928000001\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 263.6285542400001\n",
      "Updated Q-Value at index [6, 5] with value: 263.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 329.5356928000001\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 263.6285542400001\n",
      "Updated Q-Value at index [6, 5] with value: 263.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [2, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 289.8995200000001\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 231.9196160000001\n",
      "Updated Q-Value at index [4, 2] with value: 231.9196160000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [1, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 411.91961600000013\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 329.5356928000001\n",
      "Updated Q-Value at index [5, 3] with value: 329.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [8, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [2, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 429.5356928000001\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 343.6285542400001\n",
      "Updated Q-Value at index [2, 4] with value: 343.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 263.6285542400001\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 210.90284339200008\n",
      "Updated Q-Value at index [5, 6] with value: 210.90284339200008\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 411.91961600000013\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 329.5356928000001\n",
      "Updated Q-Value at index [5, 3] with value: 329.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [1, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 329.5356928000001\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 263.6285542400001\n",
      "Updated Q-Value at index [6, 5] with value: 263.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [6, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 263.6285542400001\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 210.90284339200008\n",
      "Updated Q-Value at index [5, 6] with value: 210.90284339200008\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 231.9196160000001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 185.5356928000001\n",
      "Updated Q-Value at index [8, 0] with value: 185.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 343.6285542400001\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 274.9028433920001\n",
      "Updated Q-Value at index [0, 2] with value: 274.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 411.91961600000013\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 329.5356928000001\n",
      "Updated Q-Value at index [7, 3] with value: 329.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 429.5356928000001\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 343.6285542400001\n",
      "Updated Q-Value at index [7, 7] with value: 443.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 329.5356928000001\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 263.6285542400001\n",
      "Updated Q-Value at index [3, 5] with value: 263.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 274.9028433920001\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 219.92227471360007\n",
      "Updated Q-Value at index [1, 0] with value: 219.92227471360007\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 274.9028433920001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 219.92227471360007\n",
      "Updated Q-Value at index [8, 0] with value: 219.92227471360007\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 329.5356928000001\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 263.6285542400001\n",
      "Updated Q-Value at index [6, 5] with value: 263.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 274.9028433920001\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 219.92227471360007\n",
      "Updated Q-Value at index [1, 0] with value: 219.92227471360007\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 274.9028433920001\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 219.92227471360007\n",
      "Updated Q-Value at index [2, 0] with value: 219.92227471360007\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 219.92227471360007\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 175.93781977088008\n",
      "Updated Q-Value at index [0, 1] with value: 175.93781977088008\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 429.5356928000001\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 343.6285542400001\n",
      "Updated Q-Value at index [7, 4] with value: 343.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 274.9028433920001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 219.92227471360007\n",
      "Updated Q-Value at index [8, 0] with value: 219.92227471360007\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 343.6285542400001\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 274.9028433920001\n",
      "Updated Q-Value at index [4, 2] with value: 274.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [2]\n",
      "Chosen Node/Action to explore: 2\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 2]\n",
      "Future Edge/State Q-Value Index [0, 2] has value: 274.9028433920001\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 2]: 219.92227471360007\n",
      "Updated Q-Value at index [1, 0] with value: 219.92227471360007\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 411.91961600000013\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 329.5356928000001\n",
      "Updated Q-Value at index [5, 3] with value: 329.5356928000001\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 443.6285542400001\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 354.9028433920001\n",
      "Updated Q-Value at index [3, 7] with value: 454.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 454.9028433920001\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 363.9222747136001\n",
      "Updated Q-Value at index [0, 3] with value: 363.9222747136001\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 263.6285542400001\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 210.90284339200008\n",
      "Updated Q-Value at index [5, 6] with value: 210.90284339200008\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 454.9028433920001\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 363.9222747136001\n",
      "Updated Q-Value at index [0, 3] with value: 363.9222747136001\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [8, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [8, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [8, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [1, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 329.5356928000001\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 263.6285542400001\n",
      "Updated Q-Value at index [6, 5] with value: 263.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 454.9028433920001\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 363.9222747136001\n",
      "Updated Q-Value at index [7, 3] with value: 363.9222747136001\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [3, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 263.6285542400001\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 210.90284339200008\n",
      "Updated Q-Value at index [5, 6] with value: 210.90284339200008\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 429.5356928000001\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 343.6285542400001\n",
      "Updated Q-Value at index [2, 4] with value: 343.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [6, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [8, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 443.6285542400001\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 354.9028433920001\n",
      "Updated Q-Value at index [3, 7] with value: 454.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 343.6285542400001\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 274.9028433920001\n",
      "Updated Q-Value at index [4, 2] with value: 274.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [1, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 443.6285542400001\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 354.9028433920001\n",
      "Updated Q-Value at index [7, 7] with value: 454.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 454.9028433920001\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 363.9222747136001\n",
      "Updated Q-Value at index [3, 7] with value: 463.9222747136001\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [3, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [8, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [2, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 463.9222747136001\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 371.13781977088007\n",
      "Updated Q-Value at index [5, 3] with value: 371.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 343.6285542400001\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 274.9028433920001\n",
      "Updated Q-Value at index [4, 2] with value: 274.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 371.13781977088007\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 296.9102558167041\n",
      "Updated Q-Value at index [3, 5] with value: 296.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 343.6285542400001\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 274.9028433920001\n",
      "Updated Q-Value at index [4, 2] with value: 274.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [1, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 454.9028433920001\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 363.9222747136001\n",
      "Updated Q-Value at index [7, 7] with value: 463.9222747136001\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [1, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [6, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 463.9222747136001\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 371.13781977088007\n",
      "Updated Q-Value at index [3, 7] with value: 471.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 363.9222747136001\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 291.13781977088007\n",
      "Updated Q-Value at index [6, 0] with value: 291.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 463.9222747136001\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 371.13781977088007\n",
      "Updated Q-Value at index [3, 7] with value: 471.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [5, 3] with value: 376.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 463.9222747136001\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 371.13781977088007\n",
      "Updated Q-Value at index [3, 7] with value: 471.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 429.5356928000001\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 343.6285542400001\n",
      "Updated Q-Value at index [2, 4] with value: 343.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [6, 5] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 463.9222747136001\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 371.13781977088007\n",
      "Updated Q-Value at index [3, 7] with value: 471.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [5, 3] with value: 376.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 429.5356928000001\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 343.6285542400001\n",
      "Updated Q-Value at index [7, 4] with value: 343.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [5, 3] with value: 376.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 343.6285542400001\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 274.9028433920001\n",
      "Updated Q-Value at index [0, 2] with value: 274.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [0, 3] with value: 376.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 301.5282046533633\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 241.22256372269064\n",
      "Updated Q-Value at index [0, 6] with value: 241.22256372269064\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 463.9222747136001\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 371.13781977088007\n",
      "Updated Q-Value at index [3, 7] with value: 471.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [8, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 429.5356928000001\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 343.6285542400001\n",
      "Updated Q-Value at index [2, 4] with value: 343.6285542400001\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [1, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [5, 3] with value: 376.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [8, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [7, 3] with value: 376.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 463.9222747136001\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 371.13781977088007\n",
      "Updated Q-Value at index [7, 7] with value: 471.13781977088007\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [3, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [8, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 301.5282046533633\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 241.22256372269064\n",
      "Updated Q-Value at index [0, 1] with value: 241.22256372269064\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 343.6285542400001\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 274.9028433920001\n",
      "Updated Q-Value at index [4, 2] with value: 274.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [7, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [1, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [3, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [2, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [4, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [5, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [3, 5] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [4, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 301.5282046533633\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 241.22256372269064\n",
      "Updated Q-Value at index [5, 6] with value: 241.22256372269064\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [1, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [4, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [4, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 301.5282046533633\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 241.22256372269064\n",
      "Updated Q-Value at index [5, 6] with value: 241.22256372269064\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [7, 4] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [6, 5] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [7, 4] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [1, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [2, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [3, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [3, 5] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [5, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [1, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 305.2225637226906\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 244.1780509781525\n",
      "Updated Q-Value at index [5, 6] with value: 244.1780509781525\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [8, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [7, 4] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [8, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 343.6285542400001\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 274.9028433920001\n",
      "Updated Q-Value at index [0, 2] with value: 274.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [2, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [1, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [3, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 343.6285542400001\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 274.9028433920001\n",
      "Updated Q-Value at index [0, 2] with value: 274.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [8, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 376.9102558167041\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 301.5282046533633\n",
      "Updated Q-Value at index [8, 0] with value: 301.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [7, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [7, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 8\n",
      "Max Actions to explore from node 8: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 8]. Future Edge/State to explore for Q-Learning: [8, 0]\n",
      "Future Edge/State Q-Value Index [8, 0] has value: 301.5282046533633\n",
      "Current Reward for starting state-action pair index [0, 8]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [8, 0]: 241.22256372269064\n",
      "Updated Q-Value at index [0, 8] with value: 241.22256372269064\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [7, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [0, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 305.2225637226906\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 244.1780509781525\n",
      "Updated Q-Value at index [0, 6] with value: 244.1780509781525\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 343.6285542400001\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 274.9028433920001\n",
      "Updated Q-Value at index [4, 2] with value: 274.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 343.6285542400001\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 274.9028433920001\n",
      "Updated Q-Value at index [4, 2] with value: 274.9028433920001\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [4, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [2, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 301.5282046533633\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 241.22256372269064\n",
      "Updated Q-Value at index [0, 1] with value: 241.22256372269064\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 301.5282046533633\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 241.22256372269064\n",
      "Updated Q-Value at index [0, 1] with value: 241.22256372269064\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [4, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [5, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [5, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 8\n",
      "Max Actions to explore from node 8: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 8]. Future Edge/State to explore for Q-Learning: [8, 0]\n",
      "Future Edge/State Q-Value Index [8, 0] has value: 301.5282046533633\n",
      "Current Reward for starting state-action pair index [0, 8]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [8, 0]: 241.22256372269064\n",
      "Updated Q-Value at index [0, 8] with value: 241.22256372269064\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [1, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [4, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [4, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [1, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [2, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [2, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [1, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [3, 5] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 305.2225637226906\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 244.1780509781525\n",
      "Updated Q-Value at index [5, 6] with value: 244.1780509781525\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [2, 4] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 305.2225637226906\n",
      "Updated Q-Value at index [4, 2] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [4, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [7, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [8, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [1, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [2, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 305.2225637226906\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 244.1780509781525\n",
      "Updated Q-Value at index [0, 1] with value: 244.1780509781525\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [2, 4] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [3, 5] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [7, 4] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [3, 5] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [5, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [5, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [2, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [3, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [2, 4] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 305.2225637226906\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 244.1780509781525\n",
      "Updated Q-Value at index [0, 1] with value: 244.1780509781525\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [8, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [2, 4] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [1, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [6, 5] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 471.13781977088007\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 376.9102558167041\n",
      "Updated Q-Value at index [7, 7] with value: 476.9102558167041\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [7, 7] with value: 481.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [1, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 476.9102558167041\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 381.5282046533633\n",
      "Updated Q-Value at index [0, 3] with value: 381.5282046533633\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [8, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 305.2225637226906\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 244.1780509781525\n",
      "Updated Q-Value at index [5, 6] with value: 244.1780509781525\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 305.2225637226906\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 244.1780509781525\n",
      "Updated Q-Value at index [0, 6] with value: 244.1780509781525\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 305.2225637226906\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 244.1780509781525\n",
      "Updated Q-Value at index [5, 6] with value: 244.1780509781525\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [4, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [4, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 8\n",
      "Max Actions to explore from node 8: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 8]. Future Edge/State to explore for Q-Learning: [8, 0]\n",
      "Future Edge/State Q-Value Index [8, 0] has value: 305.2225637226906\n",
      "Current Reward for starting state-action pair index [0, 8]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [8, 0]: 244.1780509781525\n",
      "Updated Q-Value at index [0, 8] with value: 244.1780509781525\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 305.2225637226906\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 244.1780509781525\n",
      "Updated Q-Value at index [5, 6] with value: 244.1780509781525\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 305.2225637226906\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 244.1780509781525\n",
      "Updated Q-Value at index [0, 6] with value: 244.1780509781525\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [1, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [2, 4] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 381.5282046533633\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 305.2225637226906\n",
      "Updated Q-Value at index [8, 0] with value: 305.2225637226906\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [3, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [7, 4] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [2, 4] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [0, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [0, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [4, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [4, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [2, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 248.43395262601769\n",
      "Updated Q-Value at index [0, 6] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [4, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [7, 4] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [7, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [4, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 248.43395262601769\n",
      "Updated Q-Value at index [0, 6] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [5, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 248.43395262601769\n",
      "Updated Q-Value at index [5, 6] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [7, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [5, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 248.43395262601769\n",
      "Updated Q-Value at index [5, 6] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [5, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [2, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 5] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 248.43395262601769\n",
      "Updated Q-Value at index [5, 6] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [2, 4] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [4, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [4, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [3, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [7, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [7, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [4, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [0, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [4, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 5]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 248.43395262601769\n",
      "Updated Q-Value at index [5, 6] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 248.43395262601769\n",
      "Updated Q-Value at index [5, 6] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [4, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [0, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 5] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [4, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [3, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [4, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [7, 4] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [4, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [3, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 248.43395262601769\n",
      "Updated Q-Value at index [5, 6] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [7, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [0, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [4, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [3, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [2, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [2, 4] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [2, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 5] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [3, 5] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [4, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [4, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 5]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 248.43395262601769\n",
      "Updated Q-Value at index [5, 6] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [0, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 5] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [0, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [2, 4] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [3, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [4, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [2, 4] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 248.43395262601769\n",
      "Updated Q-Value at index [5, 6] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [3, 5] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [7, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [2, 4] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [3, 5] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 248.43395262601769\n",
      "Updated Q-Value at index [0, 1] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 5] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [3, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [5, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 481.5282046533633\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 385.22256372269067\n",
      "Updated Q-Value at index [7, 7] with value: 485.22256372269067\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [4, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 248.43395262601769\n",
      "Updated Q-Value at index [0, 1] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [4, 7] with value: 488.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [0, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [3, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 5] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [5, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [5, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [0, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [7, 3] with value: 388.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [3, 7] with value: 488.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 310.5424407825221\n",
      "Updated Q-Value at index [4, 2] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [2, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 488.17805097815256\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 390.5424407825221\n",
      "Updated Q-Value at index [7, 4] with value: 390.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 248.43395262601769\n",
      "Updated Q-Value at index [0, 1] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 488.17805097815256\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 390.5424407825221\n",
      "Updated Q-Value at index [5, 3] with value: 390.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 248.43395262601769\n",
      "Updated Q-Value at index [0, 1] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 488.17805097815256\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 390.5424407825221\n",
      "Updated Q-Value at index [7, 4] with value: 390.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 390.5424407825221\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 312.4339526260177\n",
      "Updated Q-Value at index [6, 5] with value: 312.4339526260177\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 488.17805097815256\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 390.5424407825221\n",
      "Updated Q-Value at index [5, 3] with value: 390.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 485.22256372269067\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 388.17805097815256\n",
      "Updated Q-Value at index [7, 7] with value: 488.17805097815256\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 488.17805097815256\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 390.5424407825221\n",
      "Updated Q-Value at index [7, 7] with value: 490.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 488.17805097815256\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 390.5424407825221\n",
      "Updated Q-Value at index [7, 3] with value: 390.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 490.5424407825221\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 392.4339526260177\n",
      "Updated Q-Value at index [7, 7] with value: 492.4339526260177\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 492.4339526260177\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 393.9471621008142\n",
      "Updated Q-Value at index [4, 7] with value: 493.9471621008142\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [6, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 492.4339526260177\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 393.9471621008142\n",
      "Updated Q-Value at index [4, 7] with value: 493.9471621008142\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [1, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 492.4339526260177\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 393.9471621008142\n",
      "Updated Q-Value at index [4, 7] with value: 493.9471621008142\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 492.4339526260177\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 393.9471621008142\n",
      "Updated Q-Value at index [3, 7] with value: 493.9471621008142\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 492.4339526260177\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 393.9471621008142\n",
      "Updated Q-Value at index [3, 7] with value: 493.9471621008142\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [5, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 388.17805097815256\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 310.5424407825221\n",
      "Updated Q-Value at index [8, 0] with value: 310.5424407825221\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [5, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [0, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [7, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 492.4339526260177\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 393.9471621008142\n",
      "Updated Q-Value at index [4, 7] with value: 493.9471621008142\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 8\n",
      "Max Actions to explore from node 8: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 8]. Future Edge/State to explore for Q-Learning: [8, 0]\n",
      "Future Edge/State Q-Value Index [8, 0] has value: 310.5424407825221\n",
      "Current Reward for starting state-action pair index [0, 8]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [8, 0]: 248.43395262601769\n",
      "Updated Q-Value at index [0, 8] with value: 248.43395262601769\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [5, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [5, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 316.1261837445211\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 252.9009469956169\n",
      "Updated Q-Value at index [5, 6] with value: 252.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 492.4339526260177\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 393.9471621008142\n",
      "Updated Q-Value at index [4, 7] with value: 493.9471621008142\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [2, 4] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 316.1261837445211\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 252.9009469956169\n",
      "Updated Q-Value at index [0, 6] with value: 252.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 316.1261837445211\n",
      "Updated Q-Value at index [0, 2] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [5, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 316.1261837445211\n",
      "Updated Q-Value at index [4, 2] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 316.1261837445211\n",
      "Updated Q-Value at index [4, 2] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [7, 4] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 316.1261837445211\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 252.9009469956169\n",
      "Updated Q-Value at index [5, 6] with value: 252.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [3, 5] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 316.1261837445211\n",
      "Updated Q-Value at index [0, 2] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [5, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [3, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [7, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 492.4339526260177\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 393.9471621008142\n",
      "Updated Q-Value at index [7, 7] with value: 493.9471621008142\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [7, 4] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [5, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [2, 4] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [0, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [7, 7] with value: 495.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 495.15772968065136\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 396.1261837445211\n",
      "Updated Q-Value at index [7, 7] with value: 496.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [5, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [2, 4] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 496.1261837445211\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 396.9009469956169\n",
      "Updated Q-Value at index [7, 7] with value: 496.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [2, 4] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 316.1261837445211\n",
      "Updated Q-Value at index [4, 2] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [5, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 316.1261837445211\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 252.9009469956169\n",
      "Updated Q-Value at index [0, 1] with value: 252.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 316.1261837445211\n",
      "Updated Q-Value at index [0, 2] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 496.9009469956169\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 397.5207575964935\n",
      "Updated Q-Value at index [7, 7] with value: 497.5207575964935\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 5] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [7, 4] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [0, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 316.1261837445211\n",
      "Updated Q-Value at index [4, 2] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 316.1261837445211\n",
      "Updated Q-Value at index [0, 2] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 5] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 5] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 497.5207575964935\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.01660607719487\n",
      "Updated Q-Value at index [7, 7] with value: 498.01660607719487\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [2, 4] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [3, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [3, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [7, 4] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 8\n",
      "Max Actions to explore from node 8: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 8]. Future Edge/State to explore for Q-Learning: [8, 0]\n",
      "Future Edge/State Q-Value Index [8, 0] has value: 316.1261837445211\n",
      "Current Reward for starting state-action pair index [0, 8]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [8, 0]: 252.9009469956169\n",
      "Updated Q-Value at index [0, 8] with value: 252.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [3, 5] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.01660607719487\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.4132848617559\n",
      "Updated Q-Value at index [4, 7] with value: 498.4132848617559\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 493.9471621008142\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 395.15772968065136\n",
      "Updated Q-Value at index [0, 3] with value: 395.15772968065136\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [3, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.01660607719487\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.4132848617559\n",
      "Updated Q-Value at index [4, 7] with value: 498.4132848617559\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 8\n",
      "Max Actions to explore from node 8: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 8]. Future Edge/State to explore for Q-Learning: [8, 0]\n",
      "Future Edge/State Q-Value Index [8, 0] has value: 316.1261837445211\n",
      "Current Reward for starting state-action pair index [0, 8]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [8, 0]: 252.9009469956169\n",
      "Updated Q-Value at index [0, 8] with value: 252.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [2, 4] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 5]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 316.1261837445211\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 252.9009469956169\n",
      "Updated Q-Value at index [5, 6] with value: 252.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.01660607719487\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.4132848617559\n",
      "Updated Q-Value at index [3, 7] with value: 498.4132848617559\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 8\n",
      "Max Actions to explore from node 8: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 8]. Future Edge/State to explore for Q-Learning: [8, 0]\n",
      "Future Edge/State Q-Value Index [8, 0] has value: 316.1261837445211\n",
      "Current Reward for starting state-action pair index [0, 8]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [8, 0]: 252.9009469956169\n",
      "Updated Q-Value at index [0, 8] with value: 252.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [5, 3] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.01660607719487\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.4132848617559\n",
      "Updated Q-Value at index [4, 7] with value: 498.4132848617559\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 318.98450231152384\n",
      "Updated Q-Value at index [6, 5] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [7, 4] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [0, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [0, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 318.98450231152384\n",
      "Updated Q-Value at index [0, 2] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [5, 3] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 316.1261837445211\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 252.9009469956169\n",
      "Updated Q-Value at index [0, 1] with value: 252.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 8\n",
      "Max Actions to explore from node 8: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 8]. Future Edge/State to explore for Q-Learning: [8, 0]\n",
      "Future Edge/State Q-Value Index [8, 0] has value: 316.1261837445211\n",
      "Current Reward for starting state-action pair index [0, 8]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [8, 0]: 252.9009469956169\n",
      "Updated Q-Value at index [0, 8] with value: 252.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 318.98450231152384\n",
      "Updated Q-Value at index [3, 5] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.01660607719487\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.4132848617559\n",
      "Updated Q-Value at index [4, 7] with value: 498.4132848617559\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [5, 3] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.01660607719487\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.4132848617559\n",
      "Updated Q-Value at index [4, 7] with value: 498.4132848617559\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 318.98450231152384\n",
      "Updated Q-Value at index [6, 5] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 318.98450231152384\n",
      "Updated Q-Value at index [4, 2] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 318.98450231152384\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 255.18760184921908\n",
      "Updated Q-Value at index [0, 6] with value: 255.18760184921908\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 318.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 255.18760184921908\n",
      "Updated Q-Value at index [5, 6] with value: 255.18760184921908\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [5, 3] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [5, 3] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [5, 3] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [7, 4] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 318.98450231152384\n",
      "Updated Q-Value at index [6, 5] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 318.98450231152384\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 255.18760184921908\n",
      "Updated Q-Value at index [0, 6] with value: 255.18760184921908\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.01660607719487\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.4132848617559\n",
      "Updated Q-Value at index [4, 7] with value: 498.4132848617559\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [5, 3] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [5, 3] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 318.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 255.18760184921908\n",
      "Updated Q-Value at index [5, 6] with value: 255.18760184921908\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [3, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 318.98450231152384\n",
      "Updated Q-Value at index [4, 2] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [7, 4] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.01660607719487\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.4132848617559\n",
      "Updated Q-Value at index [4, 7] with value: 498.4132848617559\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 318.98450231152384\n",
      "Updated Q-Value at index [4, 2] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 318.98450231152384\n",
      "Updated Q-Value at index [4, 2] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 8\n",
      "Max Actions to explore from node 8: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 8]. Future Edge/State to explore for Q-Learning: [8, 0]\n",
      "Future Edge/State Q-Value Index [8, 0] has value: 316.1261837445211\n",
      "Current Reward for starting state-action pair index [0, 8]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [8, 0]: 252.9009469956169\n",
      "Updated Q-Value at index [0, 8] with value: 252.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 318.98450231152384\n",
      "Updated Q-Value at index [6, 5] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [5, 3] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.01660607719487\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.4132848617559\n",
      "Updated Q-Value at index [7, 7] with value: 498.4132848617559\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [5, 3] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [4, 7] with value: 498.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 318.98450231152384\n",
      "Updated Q-Value at index [4, 2] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 316.1261837445211\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 252.9009469956169\n",
      "Updated Q-Value at index [0, 1] with value: 252.9009469956169\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [7, 7] with value: 498.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 318.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 255.18760184921908\n",
      "Updated Q-Value at index [5, 6] with value: 255.18760184921908\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [3, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [5, 3] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.4132848617559\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 398.7306278894048\n",
      "Updated Q-Value at index [7, 3] with value: 398.7306278894048\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.7306278894048\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.98450231152384\n",
      "Updated Q-Value at index [3, 7] with value: 498.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 318.98450231152384\n",
      "Updated Q-Value at index [4, 2] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [7, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.98450231152384\n",
      "Updated Q-Value at index [4, 7] with value: 498.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 318.98450231152384\n",
      "Updated Q-Value at index [4, 2] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.98450231152384\n",
      "Updated Q-Value at index [4, 7] with value: 498.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 318.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 255.18760184921908\n",
      "Updated Q-Value at index [5, 6] with value: 255.18760184921908\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.98450231152384\n",
      "Updated Q-Value at index [4, 7] with value: 498.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [1, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [2, 4] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [7, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 398.7306278894048\n",
      "Current Reward for starting state-action pair index [3, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 318.98450231152384\n",
      "Updated Q-Value at index [3, 5] with value: 318.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.7306278894048\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.98450231152384\n",
      "Updated Q-Value at index [3, 7] with value: 498.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [2, 4] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [7, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [7, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [6, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 318.98450231152384\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 255.18760184921908\n",
      "Updated Q-Value at index [0, 6] with value: 255.18760184921908\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [8, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [5, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [2, 4] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 395.15772968065136\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 316.1261837445211\n",
      "Updated Q-Value at index [2, 0] with value: 316.1261837445211\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.98450231152384\n",
      "Updated Q-Value at index [4, 7] with value: 498.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 318.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 255.18760184921908\n",
      "Updated Q-Value at index [5, 6] with value: 255.18760184921908\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.7306278894048\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.98450231152384\n",
      "Updated Q-Value at index [3, 7] with value: 498.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [0, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.98450231152384\n",
      "Updated Q-Value at index [4, 7] with value: 498.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [7, 4] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [1, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [1, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [1, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [5, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [8, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [3, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.7306278894048\n",
      "Current Reward for starting state-action pair index [3, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.98450231152384\n",
      "Updated Q-Value at index [3, 7] with value: 498.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [5, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [8, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [8, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 5\n",
      "Max Actions to explore from node 5: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 5]. Future Edge/State to explore for Q-Learning: [5, 3]\n",
      "Future Edge/State Q-Value Index [5, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [6, 5]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [5, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [6, 5] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [1, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [6, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.7306278894048\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.98450231152384\n",
      "Updated Q-Value at index [4, 7] with value: 498.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.7306278894048\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 398.98450231152384\n",
      "Updated Q-Value at index [7, 7] with value: 498.98450231152384\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [0, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [0, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [0, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [7, 4] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [8, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 319.3500814793753\n",
      "Updated Q-Value at index [4, 2] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 2\n",
      "Max Actions to explore from node 2: [4]\n",
      "Chosen Node/Action to explore: 4\n",
      "Immediate Edge: [4, 2]. Future Edge/State to explore for Q-Learning: [2, 4]\n",
      "Future Edge/State Q-Value Index [2, 4] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [4, 2]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [2, 4]: 319.3500814793753\n",
      "Updated Q-Value at index [4, 2] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [5, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [7, 4] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 5]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 319.3500814793753\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 255.48006518350027\n",
      "Updated Q-Value at index [5, 6] with value: 255.48006518350027\n",
      "\n",
      "\n",
      "Starting Current State Node: 3\n",
      "We can try these nodes: [0 5 7]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [3, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [3, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [3, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 6\n",
      "We can try these nodes: [0 5]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [6, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [6, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [6, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [7, 4] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [7, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [7, 4] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [8, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [1, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [8, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [1, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [5, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 1\n",
      "Max Actions to explore from node 1: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 1]. Future Edge/State to explore for Q-Learning: [1, 0]\n",
      "Future Edge/State Q-Value Index [1, 0] has value: 319.3500814793753\n",
      "Current Reward for starting state-action pair index [0, 1]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [1, 0]: 255.48006518350027\n",
      "Updated Q-Value at index [0, 1] with value: 255.48006518350027\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [8, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [1, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [2, 4] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 7\n",
      "We can try these nodes: [3 4 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [7, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [7, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [7, 7] with value: 499.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [5, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [2, 4] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [8, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 499.1876018492191\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 399.3500814793753\n",
      "Updated Q-Value at index [4, 7] with value: 499.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 5]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 6]. Future Edge/State to explore for Q-Learning: [6, 0]\n",
      "Future Edge/State Q-Value Index [6, 0] has value: 319.3500814793753\n",
      "Current Reward for starting state-action pair index [0, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 0]: 255.48006518350027\n",
      "Updated Q-Value at index [0, 6] with value: 255.48006518350027\n",
      "\n",
      "\n",
      "Starting Current State Node: 4\n",
      "We can try these nodes: [2 7]\n",
      "Randomly selected node: 7\n",
      "Max Actions to explore from node 7: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [4, 7]. Future Edge/State to explore for Q-Learning: [7, 7]\n",
      "Future Edge/State Q-Value Index [7, 7] has value: 499.1876018492191\n",
      "Current Reward for starting state-action pair index [4, 7]: 100.0\n",
      "Future discounted reward for next state at Q-Table index [7, 7]: 399.3500814793753\n",
      "Updated Q-Value at index [4, 7] with value: 499.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [5, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [2, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [2, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [2, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 6\n",
      "Max Actions to explore from node 6: [0 5]\n",
      "Chosen Node/Action to explore: 5\n",
      "Immediate Edge: [5, 6]. Future Edge/State to explore for Q-Learning: [6, 5]\n",
      "Future Edge/State Q-Value Index [6, 5] has value: 319.3500814793753\n",
      "Current Reward for starting state-action pair index [5, 6]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [6, 5]: 255.48006518350027\n",
      "Updated Q-Value at index [5, 6] with value: 255.48006518350027\n",
      "\n",
      "\n",
      "Starting Current State Node: 0\n",
      "We can try these nodes: [1 2 3 6 8]\n",
      "Randomly selected node: 8\n",
      "Max Actions to explore from node 8: [0]\n",
      "Chosen Node/Action to explore: 0\n",
      "Immediate Edge: [0, 8]. Future Edge/State to explore for Q-Learning: [8, 0]\n",
      "Future Edge/State Q-Value Index [8, 0] has value: 319.3500814793753\n",
      "Current Reward for starting state-action pair index [0, 8]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [8, 0]: 255.48006518350027\n",
      "Updated Q-Value at index [0, 8] with value: 255.48006518350027\n",
      "\n",
      "\n",
      "Starting Current State Node: 8\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [8, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [8, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [8, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 2\n",
      "We can try these nodes: [0 4]\n",
      "Randomly selected node: 4\n",
      "Max Actions to explore from node 4: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [2, 4]. Future Edge/State to explore for Q-Learning: [4, 7]\n",
      "Future Edge/State Q-Value Index [4, 7] has value: 499.3500814793753\n",
      "Current Reward for starting state-action pair index [2, 4]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [4, 7]: 399.4800651835003\n",
      "Updated Q-Value at index [2, 4] with value: 399.4800651835003\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [5, 3] with value: 399.1876018492191\n",
      "\n",
      "\n",
      "Starting Current State Node: 1\n",
      "We can try these nodes: [0]\n",
      "Randomly selected node: 0\n",
      "Max Actions to explore from node 0: [3]\n",
      "Chosen Node/Action to explore: 3\n",
      "Immediate Edge: [1, 0]. Future Edge/State to explore for Q-Learning: [0, 3]\n",
      "Future Edge/State Q-Value Index [0, 3] has value: 399.1876018492191\n",
      "Current Reward for starting state-action pair index [1, 0]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [0, 3]: 319.3500814793753\n",
      "Updated Q-Value at index [1, 0] with value: 319.3500814793753\n",
      "\n",
      "\n",
      "Starting Current State Node: 5\n",
      "We can try these nodes: [3 6]\n",
      "Randomly selected node: 3\n",
      "Max Actions to explore from node 3: [7]\n",
      "Chosen Node/Action to explore: 7\n",
      "Immediate Edge: [5, 3]. Future Edge/State to explore for Q-Learning: [3, 7]\n",
      "Future Edge/State Q-Value Index [3, 7] has value: 498.98450231152384\n",
      "Current Reward for starting state-action pair index [5, 3]: 0.0\n",
      "Future discounted reward for next state at Q-Table index [3, 7]: 399.1876018492191\n",
      "Updated Q-Value at index [5, 3] with value: 399.1876018492191\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Q-Value iteration algorithm for 700 iterations\n",
    "for i in range(700):\n",
    "    \n",
    "    # For each iteration start off in some random state; remember rows are states in Q-Table\n",
    "    current_state = np.random.randint(0, int(Q.shape[0]))\n",
    "    print(f\"Starting Current State Node: {current_state}\")\n",
    "    \n",
    "    # Find potential nodes to move to\n",
    "    available_action = get_available_actions(current_state)\n",
    "    print(f\"We can try these nodes: {available_action}\")\n",
    "    \n",
    "    # Pick a potential node randomly\n",
    "    action = sample_next_action(available_action)\n",
    "    print(f\"Randomly selected node: {action}\")\n",
    "    \n",
    "    # Update Q-Table for current state-action transition\n",
    "    update(current_state, action, gamma)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.480065</td>\n",
       "      <td>318.984502</td>\n",
       "      <td>399.187602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.480065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.480065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>319.350081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>319.350081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>399.480065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>319.350081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>318.984502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>498.984502</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>319.350081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>499.350081</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>399.187602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.480065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>319.350081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>319.350081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>399.187602</td>\n",
       "      <td>399.187602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>499.187602</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>319.350081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1           2           3           4           5  \\\n",
       "0    0.000000  255.480065  318.984502  399.187602    0.000000    0.000000   \n",
       "1  319.350081    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "2  319.350081    0.000000    0.000000    0.000000  399.480065    0.000000   \n",
       "3  319.350081    0.000000    0.000000    0.000000    0.000000  318.984502   \n",
       "4    0.000000    0.000000  319.350081    0.000000    0.000000    0.000000   \n",
       "5    0.000000    0.000000    0.000000  399.187602    0.000000    0.000000   \n",
       "6  319.350081    0.000000    0.000000    0.000000    0.000000  319.350081   \n",
       "7    0.000000    0.000000    0.000000  399.187602  399.187602    0.000000   \n",
       "8  319.350081    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "            6           7           8  \n",
       "0  255.480065    0.000000  255.480065  \n",
       "1    0.000000    0.000000    0.000000  \n",
       "2    0.000000    0.000000    0.000000  \n",
       "3    0.000000  498.984502    0.000000  \n",
       "4    0.000000  499.350081    0.000000  \n",
       "5  255.480065    0.000000    0.000000  \n",
       "6    0.000000    0.000000    0.000000  \n",
       "7    0.000000  499.187602    0.000000  \n",
       "8    0.000000    0.000000    0.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Trained Q-Table:\")\n",
    "pd.DataFrame(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Q matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.162516</td>\n",
       "      <td>63.879934</td>\n",
       "      <td>79.941431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.162516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.162516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63.953145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63.953145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63.953145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.879934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.926789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.953145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.941431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.162516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>63.953145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.953145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.941431</td>\n",
       "      <td>79.941431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.967462</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>63.953145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0   0.000000  51.162516  63.879934  79.941431   0.000000   0.000000   \n",
       "1  63.953145   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2  63.953145   0.000000   0.000000   0.000000  80.000000   0.000000   \n",
       "3  63.953145   0.000000   0.000000   0.000000   0.000000  63.879934   \n",
       "4   0.000000   0.000000  63.953145   0.000000   0.000000   0.000000   \n",
       "5   0.000000   0.000000   0.000000  79.941431   0.000000   0.000000   \n",
       "6  63.953145   0.000000   0.000000   0.000000   0.000000  63.953145   \n",
       "7   0.000000   0.000000   0.000000  79.941431  79.941431   0.000000   \n",
       "8  63.953145   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "           6           7          8  \n",
       "0  51.162516    0.000000  51.162516  \n",
       "1   0.000000    0.000000   0.000000  \n",
       "2   0.000000    0.000000   0.000000  \n",
       "3   0.000000   99.926789   0.000000  \n",
       "4   0.000000  100.000000   0.000000  \n",
       "5  51.162516    0.000000   0.000000  \n",
       "6   0.000000    0.000000   0.000000  \n",
       "7   0.000000   99.967462   0.000000  \n",
       "8   0.000000    0.000000   0.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Normalized Q matrix:\")\n",
    "pd.DataFrame(Q / np.max(Q) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most efficient path:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 4, 7]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find optimal route with populated Q-Table for node 0 to 7\n",
    "current_state = 2\n",
    "steps = [current_state]\n",
    "\n",
    "while current_state != 7:\n",
    "    # Find largest Q-Value in the row and select the column index that has it\n",
    "    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[1]\n",
    "    \n",
    "    # If all same value, pick one at random\n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index, size=1))\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "    \n",
    "    # Once we have index for maximum Q-Value, append node to index\n",
    "    steps.append(next_step_index)\n",
    "    \n",
    "    # Update current state to new node, keep going until node 7 is reached\n",
    "    current_state = next_step_index\n",
    "    \n",
    "print(\"Most efficient path:\")\n",
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/anaconda3/lib/python3.6/site-packages/networkx/drawing/nx_pylab.py:611: MatplotlibDeprecationWarning: isinstance(..., numbers.Number)\n",
      "  if cb.is_numlike(alpha):\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw_networkx_nodes(G, position)\n",
    "nx.draw_networkx_edges(G, position)\n",
    "nx.draw_networkx_labels(G, position)\n",
    "nx.draw_networkx_edge_labels(G, position)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
