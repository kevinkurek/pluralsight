{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install s3fs\n",
    "\n",
    "# Have to install vowpal wabbit using the wheels in this link\n",
    "# https://github.com/VowpalWabbit/vowpal_wabbit/issues/1764#issuecomment-701379336\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "pd.set_option('max.columns', None)\n",
    "pd.set_option('max.rows',None)\n",
    "\n",
    "# data location\n",
    "# role = get_execution_role() Dont run with SM Studio, yes for NB Instances\n",
    "bucket_name = 'sagemaker-datascience-payments-dev'\n",
    "data_key = 'VW_CMAB_data_NoRetry.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket_name, data_key)\n",
    "\n",
    "# Input S3 data into dataframe\n",
    "df = pd.read_csv(data_location, index_col=0)\n",
    "print(df.shape)\n",
    "\n",
    "# Subset out manual pays\n",
    "df = df[df['if_mannual_pay']==0]\n",
    "print(f\"Length of DF for only auto pay: {df.shape}\")\n",
    "\n",
    "# Find historical probability of each arm\n",
    "df_pct = pd.DataFrame(df.groupby(['arms', 'reward'])['reward'].count()).rename(columns={'reward':'probability'})\n",
    "df_pct = df_pct.groupby(level=0).apply(lambda x: 100 * x / float(x.sum()))\n",
    "df_pct = df_pct.reset_index()\n",
    "\n",
    "# Merge original df with probability of each arm\n",
    "df_merged = df.merge(df_pct, on=['arms', 'reward'], how='inner')\n",
    "\n",
    "# Probability to decimal\n",
    "df_merged['probability'] = df_merged['probability'] / 100\n",
    "\n",
    "# Move action(arms), cost(reward), probability calculation to front as vowpal wabbit expects\n",
    "cols_to_move = ['arms', 'reward', 'probability']\n",
    "df_merged = df_merged[ cols_to_move + [ col for col in df_merged.columns if col not in cols_to_move ] ]\n",
    "\n",
    "# Adding sequential index as vowpal wabbit expects\n",
    "df_merged['index'] = range(1, len(df_merged) + 1)\n",
    "df_merged = df_merged.set_index(\"index\")\n",
    "\n",
    "# Dropping excess features already represented in data elsewhere\n",
    "df_merged = df_merged.drop(['txn_timestamp', 'application_id'], axis=1)\n",
    "\n",
    "# VW optimizes to minimize COST which is negative of reward. Therefore reward * -1 = cost.\n",
    "df_merged['reward'] = df_merged['reward'] * -1\n",
    "# display(df_merged[df_merged['arms']==310]) Yes probabilities are p & 1-p\n",
    "df_merged.head(3)\n",
    "\n",
    "# !pip install vowpalwabbit\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from vowpalwabbit import pyvw\n",
    "\n",
    "print(f\"Length of total dataframe: {len(df_merged)}\")\n",
    "print(f\"Number of Unique Arms: {len(df_merged.arms.unique())}\") # 502 unique arms\n",
    "\n",
    "feats = list(df_merged.columns)\n",
    "exclude_feats = ['pre_txn_provider', 'pre_response_code', 'direction_change_processor']\n",
    "train_feats = [x for x in feats if x not in exclude_feats]\n",
    "train_feats\n",
    "\n",
    "### Train/Learn\n",
    "\n",
    "# initialize empty history \n",
    "# (offline eval means you can only add to history when rec matches historic data)\n",
    "# history = pd.DataFrame(data=None, columns=df.columns)\n",
    "# history = history.astype({ARM: 'int32', REWARD: 'float'})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# decode_df # Chase 0-164, Stripe 165-332, Wells 333-500\n",
    "def replay_score(history, df, t, batch_size, recs):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/pdf/1003.5956.pdf\n",
    "    replay score. reward if rec matches logged data, ignore otherwise.\n",
    "    I.E. If MAB recommended 5 movies & historical viewer data showed they watched 3 of the 5 then we'd\n",
    "    only pull the rewards for the 3 movies they played; [1, 0, 1] = liked, disliked, liked for example.\n",
    "    \"\"\"\n",
    "    # reward if rec matches logged data, ignore otherwise\n",
    "    actions = df[t:t+batch_size] # 100 possible processor matches at once\n",
    "    # Core of \"Reply\": Matching our bandit policy recommendations with actual viewer content at current timestep\n",
    "    actions = actions.loc[actions[ARM].isin(recs)] # Number out of 100 movies that matched\n",
    "    actions['scoring_round'] = t\n",
    "    # add row to history if recs match logging policy\n",
    "    history = history.append(actions) # cumulatively grows as algo steps through time\n",
    "    action_liked = actions[[ARM, REWARD]]\n",
    "    return history, action_liked\n",
    "\n",
    "def train_vw(vw, train_df):\n",
    "    \n",
    "    cost_sum = 0.\n",
    "    ctr = []\n",
    "    for i in train_df.index:\n",
    "        action = train_df.loc[i, \"arms\"]\n",
    "        cost = train_df.loc[i, \"reward\"]\n",
    "        probability = train_df.loc[i, \"probability\"]\n",
    "        f0 = train_df.loc[i, 'loan_id']\n",
    "        f1 = train_df.loc[i, \"hour\"]\n",
    "        f2 = train_df.loc[i, \"dayofweek\"]\n",
    "        f3 = train_df.loc[i, \"transaction_provider\"]\n",
    "        f4 = train_df.loc[i, \"txn_duration\"]\n",
    "        f5 = train_df.loc[i, \"type\"]\n",
    "        f6 = train_df.loc[i, \"bank\"]\n",
    "        f7 = train_df.loc[i, \"transaction_amount\"]\n",
    "        f8 = train_df.loc[i, \"financed_amount\"]\n",
    "        f9 = train_df.loc[i, \"term\"]\n",
    "        f10 = train_df.loc[i, \"fico\"]\n",
    "        f11 = train_df.loc[i, \"payment_index\"]\n",
    "        f12 = train_df.loc[i, \"destination_account\"]\n",
    "        f13 = train_df.loc[i, \"if_mannual_pay\"]\n",
    "        f14 = train_df.loc[i, \"if_change_card\"]\n",
    "        f15 = train_df.loc[i, \"if_change_processor\"]\n",
    "        f16 = train_df.loc[i, \"monthly_vintage\"]\n",
    "\n",
    "        # Add to cumulative sum for each cost\n",
    "        cost_sum += cost\n",
    "\n",
    "        # Construct the example in the required vw format.\n",
    "        learn_example = str(action) + \":\" + str(cost) + \":\" + str(probability) + \" | \" + \\\n",
    "                        str(f0) + \" \" + str(f1) + \" \" + str(f2) + \" \" + str(f3) + \" \" + \\\n",
    "                        str(f4) + \" \" + str(f5) + \" \" + str(f6) + \" \" + \\\n",
    "                        str(f7) + \" \" + str(f8) + \" \" + str(f9) + \" \" + \\\n",
    "                        str(f10) + \" \" + str(f11) + \" \" + str(f12) + \" \" + \\\n",
    "                        str(f13) + \" \" + str(f14) + \" \" + str(f15) + \" \" + \\\n",
    "                        str(f16)\n",
    "    #     print(learn_example)\n",
    "    \n",
    "        # Here we do the actual learning.\n",
    "        vw.learn(learn_example)\n",
    "\n",
    "        # Add reward = -1*cost to ctr list\n",
    "        ctr.append(-1*cost_sum/i)\n",
    "        \n",
    "    return vw, ctr\n",
    "    \n",
    "def plot_ctr(num_iterations, ctr):\n",
    "    plt.plot(range(1,num_iterations+1), ctr)\n",
    "    plt.xlabel('num_iterations', fontsize=14)\n",
    "    plt.ylabel('ctr', fontsize=14)\n",
    "    plt.ylim([0,1])\n",
    "\n",
    "# 20% exploration w/ epsilon greedy policy\n",
    "vw = pyvw.vw(\"--cb_explore 502 -q UA --quiet --epsilon 0.1\")\n",
    "\n",
    "# Learn\n",
    "train_df = df_merged # [-100000:]\n",
    "vw, ctr = train_vw(vw, train_df)\n",
    "\n",
    "# Plot Learning\n",
    "num_iterations = len(train_df)\n",
    "plot_ctr(num_iterations, ctr)\n",
    "\n",
    "### Need to print CTR explicitly\n",
    "\n",
    "# Test on last 20 records\n",
    "exclude = ['arms', 'reward', 'probability']\n",
    "test_df = df_merged[ [ col for col in df_merged.columns if col not in exclude ] ]\n",
    "test_df = test_df[-20:].reset_index(drop=True)\n",
    "test_df['index'] = range(1, len(test_df) + 1)\n",
    "test_df = test_df.set_index(\"index\")\n",
    "test_df.head(3)\n",
    "\n",
    "for i in test_df.index:\n",
    "        f0 = train_df.loc[i, 'loan_id']\n",
    "        f1 = train_df.loc[i, \"hour\"]\n",
    "        f2 = train_df.loc[i, \"dayofweek\"]\n",
    "        f3 = train_df.loc[i, \"transaction_provider\"]\n",
    "        f4 = train_df.loc[i, \"txn_duration\"]\n",
    "        f5 = train_df.loc[i, \"type\"]\n",
    "        f6 = train_df.loc[i, \"bank\"]\n",
    "        f7 = train_df.loc[i, \"transaction_amount\"]\n",
    "        f8 = train_df.loc[i, \"financed_amount\"]\n",
    "        f9 = train_df.loc[i, \"term\"]\n",
    "        f10 = train_df.loc[i, \"fico\"]\n",
    "        f11 = train_df.loc[i, \"payment_index\"]\n",
    "        f12 = train_df.loc[i, \"destination_account\"]\n",
    "        f13 = train_df.loc[i, \"if_mannual_pay\"]\n",
    "        f14 = train_df.loc[i, \"if_change_card\"]\n",
    "        f15 = train_df.loc[i, \"if_change_processor\"]\n",
    "        f16 = train_df.loc[i, \"monthly_vintage\"]\n",
    "\n",
    "        # Construct the example in the required vw format.\n",
    "        test_example = \" | \" + \\\n",
    "                        str(f0) + \" \" + str(f1) + \" \" + str(f2) + \" \" + str(f3) + \" \" + \\\n",
    "                        str(f4) + \" \" + str(f5) + \" \" + str(f6) + \" \" + \\\n",
    "                        str(f7) + \" \" + str(f8) + \" \" + str(f9) + \" \" + \\\n",
    "                        str(f10) + \" \" + str(f11) + \" \" + str(f12) + \" \" + \\\n",
    "                        str(f13) + \" \" + str(f14) + \" \" + str(f15) + \" \" + \\\n",
    "                        str(f16)\n",
    "    #     print(test_example)\n",
    "\n",
    "        choice = vw.predict(test_example)\n",
    "    #     print(choice)\n",
    "        arm = choice.index(max(choice))\n",
    "        print(i, arm)\n",
    "\n",
    "# Save CMAB\n",
    "# vw.save('/root/kevink/payments/cmab_full_autopay.model')\n",
    "# del vw\n",
    "\n",
    "# Load CMAB for more predictions\n",
    "# vw = pyvw.vw(\"--cb 4 -i cb.model\")\n",
    "# print(vw.predict('| a b'))\n",
    "\n",
    "!pip install vowpalwabbit\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "pd.set_option('max.columns', None)\n",
    "pd.set_option('max.rows',None)\n",
    "\n",
    "# data location\n",
    "# role = get_execution_role() Dont run with SM Studio, yes for NB Instances\n",
    "bucket_name = 'sagemaker-datascience-payments-dev'\n",
    "data_key = 'last20txn.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket_name, data_key)\n",
    "\n",
    "# Input S3 data into dataframe\n",
    "df = pd.read_csv(data_location)\n",
    "df = df.sort_values(by='TXN_TIMESTAMP').reset_index(drop=True)\n",
    "display(df.head(3))\n",
    "\n",
    "feat_list = [\"hour\", \"dayofweek\",\"transaction_provider\",\"txn_duration\",\"type\",\"bank\",\n",
    "             \"transaction_amount\",\"financed_amount\",\"term\",\"fico\",\"payment_index\",\n",
    "             \"destination_account\",\"if_mannual_pay\", \"if_change_card\",\"if_change_processor\",\n",
    "             \"monthly_vintage\"]\n",
    "feat_list = [x.upper() for x in feat_list]\n",
    "\n",
    "# Subset to features only\n",
    "test_df = df[feat_list]\n",
    "\n",
    "# Load locally saved cmab_full.model\n",
    "from vowpalwabbit import pyvw\n",
    "vw = pyvw.vw(\"--cb 502 -i /root/kevink/payments/cmab_full_autopay.model\")\n",
    "\n",
    "predictions=[]\n",
    "for i in test_df.index:\n",
    "    f1 = test_df.loc[i, feat_list[0]]\n",
    "    f2 = test_df.loc[i, feat_list[1]]\n",
    "    f3 = test_df.loc[i, feat_list[2]]\n",
    "    f4 = test_df.loc[i, feat_list[3]]\n",
    "    f5 = test_df.loc[i, feat_list[4]]\n",
    "    f6 = test_df.loc[i, feat_list[5]]\n",
    "    f7 = test_df.loc[i, feat_list[6]]\n",
    "    f8 = test_df.loc[i, feat_list[7]]\n",
    "    f9 = test_df.loc[i, feat_list[8]]\n",
    "    f10 = test_df.loc[i, feat_list[9]]\n",
    "    f11 = test_df.loc[i, feat_list[10]]\n",
    "    f12 = test_df.loc[i, feat_list[11]]\n",
    "    f13 = test_df.loc[i, feat_list[12]]\n",
    "    f14 = test_df.loc[i, feat_list[13]]\n",
    "    f15 = test_df.loc[i, feat_list[14]]\n",
    "    f16 = test_df.loc[i, feat_list[15]] # Maybe just previous processor?\n",
    "#     f17 = test_df.loc[i, feat_list[16]]\n",
    "    \n",
    "    test_example = \"| \" + str(f1) + \" \" + str(f2) + \" \" + str(f3) + \" \" + \\\n",
    "                    str(f4) + \" \" + str(f5) + \" \" + str(f6) + \" \" + \\\n",
    "                    str(f7) + \" \" + str(f8) + \" \" + str(f9) + \" \" + \\\n",
    "                    str(f10) + \" \" + str(f11) + \" \" + str(f12) + \" \" + \\\n",
    "                    str(f13) + \" \" + str(f14) + \" \" + str(f15) + \" \" + \\\n",
    "                    str(f16)\n",
    "#     print(test_example)\n",
    "\n",
    "    choice = vw.predict(test_example)\n",
    "#     print(choice)\n",
    "    arm = choice.index(max(choice))\n",
    "    predictions.append(arm)\n",
    "    \n",
    "preds_df = pd.DataFrame(predictions, columns={'predicted_arm'})\n",
    "\n",
    "# Chase 0-164, Stripe 165-332, Wells 333-500\n",
    "\n",
    "p_df = preds_df.copy()\n",
    "# Add What Transaction Actions we actually took\n",
    "p_df['actual_provider'] = list(df['TRANSACTION_PROVIDER'])\n",
    "p_df['actual_success?'] = list(df['TRANSACTION_STATUS'])\n",
    "p_df['actual_arm'] = df['HOUR'].astype(str)+'_'+df['DAYOFWEEK']+'_'+df['TRANSACTION_PROVIDER']\n",
    "\n",
    "print(p_df.dtypes)\n",
    "p_df.head()\n",
    "\n",
    "def arm_mapping(arm):\n",
    "    if arm <= 164:\n",
    "        return \"Chase\"\n",
    "    elif arm <= 332:\n",
    "        return \"Stripe\"\n",
    "    else:\n",
    "        return \"Wells Fargo\"\n",
    "\n",
    "# Add Theoretical Provider\n",
    "p_df['pred_provider'] = p_df['predicted_arm'].apply(arm_mapping)\n",
    "p_df['processor_match'] = np.where(p_df['actual_provider']==p_df['pred_provider'], \"YES\", \"NO\")\n",
    "\n",
    "# Get decode_df of arms\n",
    "bucket_name = 'sagemaker-datascience-payments-dev'\n",
    "data_key = 'decode_arms.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket_name, data_key)\n",
    "\n",
    "# Input S3 data into dataframe\n",
    "decode_df = pd.read_csv(data_location, index_col=0)\n",
    "decode_df.columns = [str(col) + '_encoded' for col in decode_df.columns]\n",
    "decode_df['encode_arm'] = decode_df['hour_encoded'].astype(str)+'_'+decode_df['dayofweek_encoded']+'_'+decode_df['transaction_provider_encoded']\n",
    "print(decode_df.shape, decode_df.dtypes)\n",
    "display(decode_df.head(3))\n",
    "\n",
    "# Merge decoded_df so we can decode predicted arm\n",
    "p_df = p_df.merge(decode_df, left_on=['predicted_arm'], right_on=['arms_encoded'])\n",
    "p_df['pred_arm'] = p_df['hour_encoded'].astype(str)+'_'+p_df['dayofweek_encoded']+'_'+p_df['transaction_provider_encoded']\n",
    "p_df = p_df.drop(['hour_encoded', 'dayofweek_encoded', 'transaction_provider_encoded',\n",
    "                  'arms_encoded', 'actual_provider', 'pred_provider'], axis=1)\n",
    "\n",
    "p_df = p_df[['actual_arm', 'actual_success?', 'predicted_arm', 'pred_arm', 'processor_match']]\n",
    "\n",
    "\n",
    "# actual arm -> 3 columns, merged decode_df to get arms_encoded, delete old cols\n",
    "p_df[['act_hour', 'act_trans', 'act_day']] = p_df.apply(lambda x: [x['actual_arm'].split('_')[0],\n",
    "                                                                     x['actual_arm'].split('_')[1],\n",
    "                                                                     x['actual_arm'].split('_')[2]], \n",
    "                                                                     axis = 1,\n",
    "                                                                     result_type='expand')\n",
    "# Make sure same types\n",
    "p_df['act_hour'] = p_df['act_hour'].astype(int)\n",
    "p_df = p_df.merge(decode_df, \n",
    "                  left_on=['actual_arm'], \n",
    "                  right_on=['encode_arm'],\n",
    "                  how='left')\n",
    "\n",
    "# Subset to cols we care about\n",
    "p_df = p_df[['actual_arm', 'arms_encoded', 'actual_success?', 'predicted_arm', 'pred_arm', 'processor_match']]\n",
    "p_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:VW_RL]",
   "language": "python",
   "name": "conda-env-VW_RL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
