{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember you're in Conda Virtual Env: conda activate vw\n",
    "\n",
    "# !pip install data-science-types\n",
    "# !pip install tensorflow\n",
    "# !pip install keras\n",
    "# !pip install pydot\n",
    "# !pip install graphviz\n",
    "# !brew install graphviz Local Terminal for YES/NO question. # If Failure go to next line\n",
    "# !xcode-select --install # If Failure go to next line\n",
    "# https://stackoverflow.com/questions/19907576/xcode-is-not-currently-available-from-the-software-update-server\n",
    "# -> https://developer.apple.com/download/more/?name=Command%20Line%20Tools\n",
    "# -> Download top package; mine is 2020/11/05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10489432 0.19891457 0.29819024 0.39916733 0.50018615 0.60077816\n",
      " 0.7007725  0.8002165  0.89932036]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.10489432, 0.19891457, 0.29819024, 0.39916733, 0.50018615,\n",
       "       0.60077816, 0.7007725 , 0.8002165 , 0.89932036], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lstm autoencoder recreate sequence\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def reconstruct_lstm(sequence: np.ndarray):\n",
    "    # reshape input into [samples, timesteps, features]\n",
    "    n_in = len(sequence)\n",
    "    sequence = sequence.reshape((1, n_in, 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "    model.add(RepeatVector(n_in))\n",
    "    model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    # fit model\n",
    "    model.fit(sequence, sequence, epochs=300, verbose=0)\n",
    "    plot_model(model, show_shapes=True, to_file='reconstruct_lstm_autoencoder.png')\n",
    "    # demonstrate recreation\n",
    "    yhat = model.predict(sequence, verbose=0)\n",
    "    print(yhat[0,:,0])\n",
    "    return yhat[0,:,0]\n",
    "\n",
    "# define input sequence\n",
    "seq = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "reconstruct_lstm(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0.10826182],\n",
       "         [0.20552245],\n",
       "         [0.30258575],\n",
       "         [0.40007335],\n",
       "         [0.4985259 ],\n",
       "         [0.59798217],\n",
       "         [0.698382  ],\n",
       "         [0.799459  ],\n",
       "         [0.901271  ]]], dtype=float32),\n",
       " array([[[0.16565828],\n",
       "         [0.28881192],\n",
       "         [0.4027717 ],\n",
       "         [0.50927055],\n",
       "         [0.61052614],\n",
       "         [0.7072726 ],\n",
       "         [0.8001721 ],\n",
       "         [0.8898257 ]]], dtype=float32)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lstm autoencoder reconstruct and predict sequence\n",
    "from numpy import array\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def composite_lstm(seq_in: np.ndarray):\n",
    "\n",
    "    # reshape input into [samples, timesteps, features]\n",
    "    n_in = len(seq_in)\n",
    "    seq_in = seq_in.reshape((1, n_in, 1))\n",
    "    # prepare output sequence\n",
    "    seq_out = seq_in[:, 1:, :]\n",
    "    n_out = n_in - 1\n",
    "    \n",
    "    # define encoder\n",
    "    visible = Input(shape=(n_in,1))\n",
    "    encoder = LSTM(100, activation='relu')(visible)\n",
    "    \n",
    "    # define reconstruct decoder\n",
    "    decoder1 = RepeatVector(n_in)(encoder)\n",
    "    decoder1 = LSTM(100, activation='relu', return_sequences=True)(decoder1)\n",
    "    decoder1 = TimeDistributed(Dense(1))(decoder1)\n",
    "    \n",
    "    # define predict decoder\n",
    "    decoder2 = RepeatVector(n_out)(encoder)\n",
    "    decoder2 = LSTM(100, activation='relu', return_sequences=True)(decoder2)\n",
    "    decoder2 = TimeDistributed(Dense(1))(decoder2)\n",
    "    \n",
    "    # tie it together\n",
    "    model = Model(inputs=visible, outputs=[decoder1, decoder2])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    plot_model(model, show_shapes=True, to_file='composite_lstm_autoencoder.png')\n",
    "    \n",
    "    # fit model\n",
    "    model.fit(seq_in, [seq_in,seq_out], epochs=300, verbose=0)\n",
    "    \n",
    "    # demonstrate prediction\n",
    "    yhat = model.predict(seq_in, verbose=0)\n",
    "#     print(yhat)\n",
    "    \n",
    "    return yhat\n",
    "    \n",
    "# define input sequence\n",
    "seq_in = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "composite_lstm(seq_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Classification LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 104s 267ms/step - loss: 0.4372 - accuracy: 0.7954\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 103s 264ms/step - loss: 0.2909 - accuracy: 0.8823\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 100s 256ms/step - loss: 0.2479 - accuracy: 0.9036\n",
      "Accuracy: 87.24%\n"
     ]
    }
   ],
   "source": [
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0931007]\n",
      " [0.9749841]\n",
      " [0.8212079]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test[:3]))\n",
    "# print(model.predict_classes(X_test[:3])) # Deprecated in favor of below\n",
    "print( (model.predict(X_test[:3]) > 0.5).astype(\"int32\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with Dropout for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "\n",
    "\n",
    "class SeqLstm:\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def seq2class(self, epochs=2, batch_size=100, dropout=0.2, lstm_layer=100, verbose=0):\n",
    "\n",
    "        # Truncate or pad input sequences with 0's prior to Keras fitting\n",
    "        self.X_train = sequence.pad_sequences(self.X_train, maxlen=max_review_length)\n",
    "        self.X_test = sequence.pad_sequences(self.X_test, maxlen=max_review_length)\n",
    "\n",
    "        # create the model\n",
    "        embedding_vector_length = 32\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(LSTM(lstm_layer))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        # Fit Model\n",
    "        model.fit(self.X_train, self.y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "        if verbose:\n",
    "            # Summary of model\n",
    "            print(model.summary())\n",
    "            # Final evaluation of the model\n",
    "            scores = model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "            print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "        return model\n",
    "    \n",
    "ex = SeqLstm(X_train, y_train, X_test, y_test)\n",
    "model = ex.seq2class(epochs=1, batch_size=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "260\n",
      "603\n",
      "500\n",
      "500\n",
      "500\n",
      "[[0.44343093]\n",
      " [0.5306598 ]\n",
      " [0.48315963]]\n",
      "Predicted results: [[0]\n",
      " [1]\n",
      " [0]]\n",
      "Actual results: [0 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Demo Records\n",
    "sample = X_test[:3]\n",
    "for i in sample:\n",
    "    print(len(i))\n",
    "\n",
    "# Preprocess sample by padding;  The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content\n",
    "new_sample = sequence.pad_sequences(sample, maxlen=max_review_length)\n",
    "for i in new_sample:\n",
    "    print(len(i))\n",
    "\n",
    "# Predict probabilities and classes\n",
    "print(model.predict(new_sample))\n",
    "print(f\"Predicted results: {(model.predict(new_sample) > 0.5).astype('int32')}\" )\n",
    "print(f\"Actual results: {y_test[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
